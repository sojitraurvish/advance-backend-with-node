// (see pic 1) this is what we are going to learn 

// server

// 1) cluster module and horizontal scaling 
// 2) capacity Estimation, ASGs and vertical scaling 
// 3) load balancers

// Database

// 1) indexing 
// 2) normalization
// 3) sharding


// 1) vertical scaling (see pic 2) here you have only single thread or proceess so your single thread can not take advantage of multi core cpu
// multi threaded languages can make efficient use of multi core cpu (see pic 3) in java create thread poll, rust you have spone thread over there, in golang it allow you to create subroutines(or coroutines) that let you create small routines that can be run on diff threads, but it is not very easy e.g if you to find some of 1000000000 then you can divide this in four parts then give each pair part to 1 thread and once it get request merge all for result so it is bit difficult because you have to manage thread by your self

// now lets see how cpu is getting useed  write this infinite code (see pic 4)
// htop stats to see cpu process

// horizontal scaling (see pic 5) also see 1_horizontal_scaling_code.js

// capacity estimation(see pic 7)
// 1) how would you scale your server -> if user increases
// 2) how do you handle spikes -> your have 1 user in next min you have 1m user 
// 3) how can you support a certain SLA given  some traffic (SLA - service level aggrement -> if use aws,or roser pay, so they will give you SLA, they say 99.9%  we will be up it mean in year we goes down for 30 mins 99.9% is our sla and if you goes down bellow that you do not need to pay us if we do not follow our sla)

// 1) paper math -> if the sla is 99.9% and maximum spike can be expacted is 1m user then i always should some server is ruuning  or i should every 3 mins check should i scale up, should i scale up..... if there is a spike that happens during crecket match happens 2 times in year, then during those 2 times my infrastructure will wait for three mins before it scale up a lot so those are the things you should thought of i say in capacity estimation you need to put things like these

// IPL happens twice in a year my SLA is 99.9% which mean my down time at max 30mins in year which means whenever this spike happen i have recover at max in 15 mins if i do not recover in 15mins then i faild to follow my SLA then whatever the logic of mine which auto scale the server that brings up a lot of infrastructure need to keep checking probebly every 5 mins, is there a spike, do i need to auto scale whenever spick happen, or ipl like spike happen what to do is you goes to bunch of providers, the ipl bandwidh that is expected to be handle in india(20 cr people is watching), it can not be handle by single cloud provider only aws can not have inough wires in india to handle that kind of bandwidth so you have multiple wanders available whenever the spike happens how can i in 15 mins quickly start those wanders so these are the thing you talk about when you do capacity estimation

// 2) estimating reqquest- there are some of the opsations in these interviews with the per second request, per second bandwidth, so you know, if you will say if they ask you how many requst do you think come in day you say there are like 1m user comes in day so they ask you how many requst each user will send you say may be 10 requsets so there is 10m requst in day so what is that translate to second so you have to do 10m / (3600 * 24) so these are the requst that i need to support per second, so let say that no is 100 requst per second  then you have to be like how much can my infrastructure support if i have single node js machine in aws how much it can resonabally handle in 1 second let say the ans is 10requst(it is not 10 for node js) but let say the ans is 10 requst per second(that my node js single server can handle) what your node js process can handle which tells you what i need to have 10 machines available beacause i need to handle 100 request a second so that is how you will arrive at the first number so for an application like this assuming 


// 3) (then they you about sale up and down in spike) Assuming / monitoring how many requests a single machine can handle - assuming you have monitoring  that tells you how many req are comming and you have some how speat out taht current no of requst per second in last 10 mins was this number 10,000 so you can use this number to scale up or down your infrastructure

// 4) autoscaling machines based on the load  that is estimated  from time to time - from time to time you need to check that what is the current load on these 10 machines and this load becomes a lot then you have to scale up if this load become really less then you have to scale down 

// so final architecture looks like this (see pic 7) - let say you have 4 machine on aws all four reciving req from users, and every few mins they need to some how aggregate that in last 10 mins i recieve 100 thousand req aws S1, s2 1m requst, s3 1m req, s4 1m req, they some how reaches a system how they reaches to system(there are multiple ways you do not need to synchronosy do this you do not need to be like http requst came, let me send the http requst  to tell the other side req has come not like that you aggrigate them, so whenever 1000 requst came you tells the aggrigated service that 1000 req came) so that how you can tell the aggrigated service that in last 10 mins these many requst came, you may spite this number out  in some database or some process which will be like ovv the number want up it will tell the auto scalling group  that you need to scale up and bring 10 more servers the load in last 10 mins increased by a lot, simillary if this load decreased by lot the this final number becomes 10 request per second then this process, whatever this process scalling up and down......... for scale down, we will implement this auto scaling group in aws latter on but this is how it works, but this is now multiple machines sending data to the single service which is aggrigating total number of requst per second, and it sends to the process that scale up and down every 10 mins

// (this is little ugley or easy way then above but this is also good to implemebt) you tell auto scaling groups that if you cps uses goes above 50% up to 80% then scale up the infrastucture create one more server so that this 80% number goes to 50% and anther thing you tells is if it ever drops to 10%  the kepp only one server on terminate all other in ASG you also pull minimum sever number which should be up and runing always  this is the most good way to do auto scaling but when you are in system design interview then they give you some wired use cases for example, one use case the matric is on request per second (we did it above) they say if this single process can handle 10 req per second and they will say so how to build a system that scales up and down based on the number of requst so your ans should be above that every some some second let say 5 mins we check for auto scaling based on req number(read above)

// now let's go for slightly complecated way(see pic 8) - what if they ask you that now you have real time application you have chess like application how would you scale now and in a chess like application you do not have req per second you have persistant connections  if there are 100 thousond people active on website, they are not sending you a lot of messages, the number of messages they are sending is low but they have persistant connection to your servers  and persistant connection are expensive you can only have 10,000 persistant connection there are some matix like this for node js process if you have more than 10,000 sockets open then you run into issues, which means for this real time application the matic that you are woried about is not the no of req but the no of people are currently active on the website, if i goes to chess.com you will see they have aggrigated this no that currently (see pic 9) this many people are active and playing the chess, if you goes to bunch of live streaming websites like twich they lot of time have on top right corner that currently 8000 people active on there website so you always have this number how this aggrigated is diff question in distributed system like this this is also a chellange that like this machine has 100 connections currently this has 1000, this has 300, this has 400 how do you aggrigate this ans remain same you create aggrigation service like this any time a persong connection S1 tell to Aggri service that i have 100 people now same for disconnects, so all the servers gives there active user no to aggrigate service send it to db so frontend users can see it  and this agg service will also try to find a average that if 88,000 people are live we have 4 servers curretly which mean each server has 20,000 open sockets i need to increase the no of servers because i know our single machine can handle only 10000 at time so it will tell auto scaling group please scale up to 8 servers, one very challing things happens here compare to the last usecases if you have 20000 people at S1 and you scale up then you need to move 10,000 users to the other servver, which requires lot of client side logic which mean on your website react code you write this logic to disconnect and reconnect to fresh websocket server the other problem comes when you scale down you have to move conneted people to one server to other server so this is one thing become challanging if you have websocket connections ------>
// -------> this not challange for http servers for http server req go req finishes fi server dies it fines new req route to other on server but it become problem for websocket server


// the theird veard usecase is video transcodder