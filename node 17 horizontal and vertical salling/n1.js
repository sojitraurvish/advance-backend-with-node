// (see pic 1) this is what we are going to learn 

// server

// 1) cluster module and horizontal scaling 
// 2) capacity Estimation, ASGs and vertical scaling 
// 3) load balancers

// Database

// 1) indexing 
// 2) normalization
// 3) sharding


// 1) vertical scaling (see pic 2) here you have only single thread or proceess so your single thread can not take advantage of multi core cpu
// multi threaded languages can make efficient use of multi core cpu (see pic 3) in java create thread poll, rust you have spone thread over there, in golang it allow you to create subroutines(or coroutines) that let you create small routines that can be run on diff threads, but it is not very easy e.g if you to find some of 1000000000 then you can divide this in four parts then give each pair part to 1 thread and once it get request merge all for result so it is bit difficult because you have to manage thread by your self

// now lets see how cpu is getting useed  write this infinite code (see pic 4)
// htop stats to see cpu process

// horizontal scaling (see pic 5) also see 1_horizontal_scaling_code.js

// capacity estimation(see pic 7)
// 1) how would you scale your server -> if user increases
// 2) how do you handle spikes -> your have 1 user in next min you have 1m user 
// 3) how can you support a certain SLA given  some traffic (SLA - service level aggrement -> if use aws,or roser pay, so they will give you SLA, they say 99.9%  we will be up it mean in year we goes down for 30 mins 99.9% is our sla and if you goes down bellow that you do not need to pay us if we do not follow our sla)

// 1) paper math -> if the sla is 99.9% and maximum spike can be expacted is 1m user then i always should some server is ruuning  or i should every 3 mins check should i scale up, should i scale up..... if there is a spike that happens during crecket match happens 2 times in year, then during those 2 times my infrastructure will wait for three mins before it scale up a lot so those are the things you should thought of i say in capacity estimation you need to put things like these

// IPL happens twice in a year my SLA is 99.9% which mean my down time at max 30mins in year which means whenever this spike happen i have recover at max in 15 mins if i do not recover in 15mins then i faild to follow my SLA then whatever the logic of mine which auto scale the server that brings up a lot of infrastructure need to keep checking probebly every 5 mins, is there a spike, do i need to auto scale whenever spick happen, or ipl like spike happen what to do is you goes to bunch of providers, the ipl bandwidh that is expected to be handle in india(20 cr people is watching), it can not be handle by single cloud provider only aws can not have inough wires in india to handle that kind of bandwidth so you have multiple wanders available whenever the spike happens how can i in 15 mins quickly start those wanders so these are the thing you talk about when you do capacity estimation

// 2) estimating reqquest- there are some of the opsations in these interviews with the per second request, per second bandwidth, so you know, if you will say if they ask you how many requst do you think come in day you say there are like 1m user comes in day so they ask you how many requst each user will send you say may be 10 requsets so there is 10m requst in day so what is that translate to second so you have to do 10m / (3600 * 24) so these are the requst that i need to support per second, so let say that no is 100 requst per second  then you have to be like how much can my infrastructure support if i have single node js machine in aws how much it can resonabally handle in 1 second let say the ans is 10requst(it is not 10 for node js) but let say the ans is 10 requst per second(that my node js single server can handle) what your node js process can handle which tells you what i need to have 10 machines available beacause i need to handle 100 request a second so that is how you will arrive at the first number so for an application like this assuming 


// 3) (then they you about sale up and down in spike) Assuming / monitoring how many requests a single machine can handle - assuming you have monitoring  that tells you how many req are comming and you have some how speat out taht current no of requst per second in last 10 mins was this number 10,000 so you can use this number to scale up or down your infrastructure

// 4) autoscaling machines based on the load  that is estimated  from time to time - from time to time you need to check that what is the current load on these 10 machines and this load becomes a lot then you have to scale up if this load become really less then you have to scale down 

// so final architecture looks like this (see pic 7) - let say you have 4 machine on aws all four reciving req from users, and every few mins they need to some how aggregate that in last 10 mins i recieve 100 thousand req aws S1, s2 1m requst, s3 1m req, s4 1m req, they some how reaches a system how they reaches to system(there are multiple ways you do not need to synchronosy do this you do not need to be like http requst came, let me send the http requst  to tell the other side req has come not like that you aggrigate them, so whenever 1000 requst came you tells the aggrigated service that 1000 req came) so that how you can tell the aggrigated service that in last 10 mins these many requst came, you may spite this number out  in some database or some process which will be like ovv the number want up it will tell the auto scalling group  that you need to scale up and bring 10 more servers the load in last 10 mins increased by a lot, simillary if this load decreased by lot the this final number becomes 10 request per second then this process, whatever this process scalling up and down......... for scale down, we will implement this auto scaling group in aws latter on but this is how it works, but this is now multiple machines sending data to the single service which is aggrigating total number of requst per second, and it sends to the process that scale up and down every 10 mins

// (this is little ugley or easy way then above but this is also good to implemebt) you tell auto scaling groups that if you cps uses goes above 50% up to 80% then scale up the infrastucture create one more server so that this 80% number goes to 50% and anther thing you tells is if it ever drops to 10%  the kepp only one server on terminate all other in ASG you also pull minimum sever number which should be up and runing always  this is the most good way to do auto scaling but when you are in system design interview then they give you some wired use cases for example, one use case the matric is on request per second (we did it above) they say if this single process can handle 10 req per second and they will say so how to build a system that scales up and down based on the number of requst so your ans should be above that every some some second let say 5 mins we check for auto scaling based on req number(read above)

// now let's go for slightly complecated way(see pic 8) - what if they ask you that now you have real time application you have chess like application how would you scale now and in a chess like application you do not have req per second you have persistant connections  if there are 100 thousond people active on website, they are not sending you a lot of messages, the number of messages they are sending is low but they have persistant connection to your servers  and persistant connection are expensive you can only have 10,000 persistant connection there are some matix like this for node js process if you have more than 10,000 sockets open then you run into issues, which means for this real time application the matic that you are woried about is not the no of req but the no of people are currently active on the website, if i goes to chess.com you will see they have aggrigated this no that currently (see pic 9) this many people are active and playing the chess, if you goes to bunch of live streaming websites like twich they lot of time have on top right corner that currently 8000 people active on there website so you always have this number how this aggrigated is diff question in distributed system like this this is also a chellange that like this machine has 100 connections currently this has 1000, this has 300, this has 400 how do you aggrigate this ans remain same you create aggrigation service like this any time a persong connection S1 tell to Aggri service that i have 100 people now same for disconnects, so all the servers gives there active user no to aggrigate service send it to db so frontend users can see it  and this agg service will also try to find a average that if 88,000 people are live we have 4 servers curretly which mean each server has 20,000 open sockets i need to increase the no of servers because i know our single machine can handle only 10000 at time so it will tell auto scaling group please scale up to 8 servers, one very challing things happens here compare to the last usecases if you have 20000 people at S1 and you scale up then you need to move 10,000 users to the other servver, which requires lot of client side logic which mean on your website react code you write this logic to disconnect and reconnect to fresh websocket server the other problem comes when you scale down you have to move conneted people to one server to other server so this is one thing become challanging if you have websocket connections ------>
// -------> this not challange for http servers for http server req go req finishes fi server dies it fines new req route to other on server but it become problem for websocket server


// the theird veard usecase is video transcodder or replit like website or you even can say letcode but not really because litcode submition happens in 2s, in replit where you have to give access to user some compute for sometime 2 hours and video transcoding also sometimes takes 2 hours. what are these three usecases, the way you scale all of threes are very similar, video transcoding mean  when you goes to youtube.com upload video, you only give youtube single mp4 file and youtube covert it into various qualities, 1080, 720p, 360p so and and so for.... this process called video transcoding (it is conveting your mp4 file in various qualities) this can take a long time, this is very expensive opration what mean by expensive, i mean you know conveting 1080 file into 3 verients will take lot of cpu that the machine you are runing on so you know if suddendly 100 people comes and upload video even 100 as number is very small only 100 people is uploading but sill you need lot of compute for this, you need almost 100 decently size machines because it takes a lot of time for this to happen so the question is how do you scale the system like this where you know that you should have compute ready for a long time because video trascoding takes 2 hours and if you goes to replet ever and try to create new reple then also you have access to replet machine because you have terminal where you run bunch of command there and that is runing someone else machine and same happens with let code when you submit it runs on someone elses machine but letcode has very less time so this architecture is not valid for litcode but for other two whenever people comes to your website and say i have uploaded a video or i want to create a reple then you have to give access to some compute imidiatly, what mean compute - you have to give access to some 2gb, 2cpu machine so this is excusive for you for two hours, when you are doing mp4 transcodding to 3 diff varients it is very expensive opration and it will take really long time so when user is uploding video then they need some part of your compute(you have pull of machine), when first user comes you allocate 1 machine, when second user comes you alloccate 2, but if you keep 100 server always on and waiting for request then it is very costly, so there are multiple ans here 1 is please auto scale, you can do is that whenever a new req comes to create new mp4 file or to upload mp4 file,------->
// ----> you can have worm poll of server( what worm pull mean - they are ready but there is noting running in it, they are ready to get the requst form youtube uploader service that if someone uploaded mp4 i will now handle it, if another person upload, then seocnd server say i will handle it and as these req keep on increasing you increase worm pull as well, you always have let say 20 machines ready that are worm(ready to pick up requsts) if you know, peak you only get 20 uploads/ in 1 min, if you know  at max you sla is, like if you send me 20 uploads in min i will handle it if you send me 21 i might not be able to handle it but my sla clearly states that you can send 22 in min  and i will be able to handle it so you need a worm pool of 20 at all times so that as people sends a request so start incresing worm pool size you keep assigning machine them one by one, why do you need to assign user entire single machine because this opration of video trascoding is very costly and same thing happen for replet utill your user sesstion got colosed) so this is one way, but what is problem with is approach  1) you are mantaining a worm pool you have 20 machines always runing which might remain ideal so you always paying for 20 machines not problem at scale, if have lot of machine if you have 1000 users then it's fine you pay for extra 20 machine when you are creating 1000 machine every month so the coat is not anyting compare to you payting if you have throwout the month you starting 2000 machines do not matter you have 20 in worm pool for a long time so that the first problem what is the second problem resource sharing is also short of problmatic, you are allocating each user to complete cpu of 2 core even though he is using only single core cup power like 1080p to 1080p take 100% cpu but 1080p to 360p do not takes a full cpu, so would not it be nice it these computes or machine shares so assume i can have 20 cpus that can be shared by 30 vides because if one vieo reaches to 360p so it does not takes full cup but here you not sharing the cpu so this is downside of this approach but still you can use in site like replet or youtube  then what is good approach , the good approch that we discuss many time  that any time video transcode happen you maintain in a queue and then you have workers that asyncly pick these tasks from queue and if queue length become very big is when you scale up your servers so if 100 people submits video then you slowly can scale up,----->
// ---->  and if length goes down of queue then you scale down the worker of server, but the problem with this architecture is it works for video uplods  because for video upload youtubes says you please waite for 1 hour for the video is processing so they have that kind fo expectaion sla with you but this is not true for replet when you goto replet when you go to replet if 1000 people try to create replet at same time then they does not have worm pool of 1000 people they usally does not get spike of 1000 people if all of us right now goes on replet then most probeally cresh replet because they does not have such a big worm poll and the do not use this queue based architecture because this is slow because if in queue you have only 3 workers then each worker pick slowly slowly and in replet you can not tell user that wait for 10 second we arrange compute for you, so these architecture will not work for replet then what will work for replet 1) orignal worm pool atchitecture or 2) cubernatic atchitecure that i discuss in youtube video

// when you have cricket match cricket match is happening in mumbai(then the video trascoding this does nto need to scale) a single stream go from the mumbai server to some server (data center) and this converts the incomming stream which genrally RTMP  to Hls stream the problm comes is that how do you send this to so many people( so the scaling problem that we discuss above is not just about the sending a data is is not like lot of people will ask for lot of data, it is more about  like lot of people will say i want to sign up sing in, or do somting like ecomm website route handlers ) if you have a creicket match is happening then there are lot of people is asking for 720p video and which is lot for bandwidth and india can support these much bandwith so how do you send these to all prectical perposes mp4 video or hls is better format that are used these websites, getting RTMP stream and transcoding into HLS format is easy bit and creating 3 qualities out of it is the easy bit the hard bit is to sending it to so many people how do you scale that and the answer surprisingly over there is also ASGs there is video on youtube where hotstar discusses this that we discuss in next video that how to scale up arcitecture in spick with ASGs. 
// https://www.youtube.com/watch?v=9b7HNzBB3OQ

// sla calculation (see pic 10)

// can chess game can be build by http server rather than websocket server

// 1) if you sending lot of data use webscokets so you do not have to do three way hand shake as you do in http 
// 2) in chess you have to push server side events to client, you can do this in http by polling but it's ugaly way of doing tahat why use websockets