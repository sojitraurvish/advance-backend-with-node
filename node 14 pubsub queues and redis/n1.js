
// websockets are not that much used for backend to backend communication they are really user for forntend to backend communication

// and these to are mostly use for backend to backend communication, it will help two diff backend to each other let dive deep in pubsub and messaging queues  

// today we will dive deep in leet code example (see pic 1)
// 1 ) queues
// 2 ) pub sub
// 3 ) redis

// we will need docker to start radis or else you can use avin to use redis instance online(but docker option is good it gives you bunch of utilities)

// why we learingin about radis beacuse redis give you a bunch of things, and two of that things are pubsub and queues, then can i do pubsub without redis, yes you can do there are many library, packages, and implementations that let you do pub subs (rabitmq is good example of queues) (sms is good emample of pubsub not technically and it is service of aws) and there are verious implementation of pubsub and redis is one of them that let you do pubsub, what are pubsub?

// now lets understand in a system like lit code where do you need queues and where do you need pubsub , the final achitecture is bit complex(with queue and pubsub) (see pic 2) but we get there soon but let look at fist example with just a queue(see pic 3) the architecture says anytime a user is on your plateform, so supporse the user is on let code and try to submit the problem the req would goes to the primary backend or yours(see pic 3), what is the primary backend user simple express server recieve the request from the browser that problem id 1 or  in this case problem id two-sub(see pic 4), code is some code snipit and the language so this input reaches the primary backend and this pushes this into queue, why it is pushes into queue can we execute the code on primary server here only and tell them the response that your code is succeed or failed, the reason why you deligate this out in  queue is that what if the user sends you the code that is malicious it may give you infinite loop so your main server may get bussy by processing single request (which should also server request to other users), and that why we deligate code to some other places and that why we have worker nodes that will process your request(why it is called workder nodes beacause there job is to pick things work on it submit that work and pick another work), so primary backed puts those taskes into queue and your worker picks the problem form queue, but why we need queue, can not my primary backend directly tell worker nodes that do this job and that why i do not have any sequrity issue on my primary server the reason you can not do that is 100 people sends you the request if 100 people sends you the code that you do not want to send up same worker(W1 - which has only 4gb space), you want to garenty, whenever you are only litcode, litcode geneties that your code will run on 2core machine with this much cpu and all, you have to make sure that garenties are mit and you have make sure every worker run single code at time and queues become very popular usecase in this case, even if you have only two workers and 20 people submits the problem you will have long queue and worker will pick slowly, a lot of people may be waiting for response but you can garenty that worker one pic 1 process at time, worker 2 pick 1 process at time, it will run it store the response somewhere and then it will pick the next one, so to make sure you pick the things one by one -------->
// ----------------> and to make sure your if primary backend is overwhelemed if everyone is trying to do a very expensive opration very fast the you can push that opration in queue and slowly we will do process that with one of the workers, the another banifit of queue is if you have 0 workers still pleople sumitions stays in queue and whenever you bring a worker it will start pickuping from queue, another benifit is you can auto scale your workers you can say like if your queue length become 100 i should start new empty worker and if queue length become 3 then i should stop some of them and say only one worker will stay and handle all the request (so you can auto scale your workers up and down), this is become supper important if on litcode there are 100 people on your platefrom you do not want to alive 100 worker at time because these are very expensive machines so you need to make sure you need to auto scale them very aggresively and here queue is good use case and that is the one usecase of queue where you might want to use queues, generally whenever you have an expensive operation that need to run on single machine, another example of this video transcoding(what is video transcoding - if i go to youtube and upload a new video it takes some time video to processes i upload a single 10mb video that video get converted into 4 - 5 diff qualities(see pic 5) this is called video transcodding this is also very expensive opration here also you want that whenever user uploads video on youtube it goes into a queue then workers eventually pick up and transcode the video ) so that is another use case where queue makes lot more sance

// the good example of queue is rabit mq, sqs simple queue in aws, or redis also let's you do queues 

// Note - whenever user want to do expensive opration on your machine a log runing expensive opration(submiting problem like lit code or upload video for transcodding) you probebally wnat to use architecture like this(see pic 3) queue and you want to upscale and downscale the workers based on the length of the queue that is when you need queues

// and when you need pubsub or publisher subscriber even before that lets learn what is pubsubs

// pubsub - let look back to our pevious example (see pic 3) user submit some code, sends it to primary backend, it sends it to queue and from there it get pick up by one of the worker process, once it picked up the worker need to tell the browser that you have accepte or rejected but it need to send the final result to the browser if remember from yesterday how does letcode does it lit code ueses polling (see pic 7), once i submit the code or submit request it start to send check request again and again, poll the serve again and again is the submition(result) done and get pending state(see pic 9) and right now it falild for this specific error(see pic 9) so let code uses polling but can you do someting better can you use websockets that we learn in last lecture, but what is the benifit of using websocket here, if use websockets then your browser won't constantly poll the backend like (is is happen...,is is happen...,is is happen...,is is happen..., and so on untill not finished) it won't overwalmed your primary backend and which will internally overwalmed your database because every step you are writing to your db as well, whenever you make submition entry goes to db and says this problem is processing, whenver worker done it update entry that it is done or rejcted but when the user long polling then it is overwheming backend again and again, but what if a server(worker where code is processing) could push that bro i am done and tell browser so it can show to user for pushing event from server to client what we use we discuess yester day we use websockets so want can happen after the worker is done, after worker is done processing your code, getting response, checking if it is correct it can send the response to websocket server(see pic 10) which is connected to a browser, whenever i goes to litcode what happen if there is persistant connection to websocket server (if i am on this page see pic 11) what if along with http request that iam sending there pesitsnt websocket connection to websocket server anytime i click one run the worker finally process it(see pic 10) the worker can singnal to the websocket layer  please tell user with id 1 if he connected to you if user with id 1 seleted to you tell them that the status of the recent submition is TLE(time limt excied) that is how worker can talk to websocket server and publish event to browser, in (pic 9) you can see that between worker and  websocket server we used pub/sub but why we used that -------->
//  why can not worker directly tell websocket server, why can not worker directly talk to browser, why we are complicating this flow, 1) worker can not directly connect to browser worker are very transitive, they come up, goes down, they should never be expose to internet , the worker job is that bro give me code i will run, put the entry in database and jayada se jayada i will publish to pubsub i will not do anything more, (and these worker very frequetly goes up and down that why you do not want your end user to connect it with directly),  so you have fresh service(mean node js application) that browser connect to, wo your browser connects to this websocket layer and whenever the worker completes the submition it can tell pubsub that who one with user id 1 please send them this specific status(see pic 9) for this submition,  but why we need pubsub why our worker can not directly tell websocket layer via http but why we need here pubsub and answer already discussed at the end of the yesterday class --answer-->(see pic 12) that in real word you have multiple websocket server not only one,(one websocket server can support only 10,000 request), so you have multiple websocket server and your user connected to one ws server out of all of them, let say user1 has persistant connection to ws3, so whenever your worker is done it does not know that should i send this information to ws server 1, ws2, or ws3, instead that your worker publish a event to pubsub and whenever user connects to websocket layer(whenever there is conection between user id 1 with websocket server3 ) (user1 throw ws3)it can subscribe to an event for user id 1(event called may be userid 1 ) in pubsub and worker can publish if worker know there is submition for userid 1, that way this worker directly reach to websocket layer (vs that fugering out that are you the ws server which is connected to user 1), but with pubsub our worker says i will publish to pubsub whoever(websocket server) wants it please subscribe to pubsub,(see pic) it might happen that you have two browser tab open or you have mobile app open and the mobile app is connected to ws1 server(you sared letcode acc to your friend in maharastra and you are using web in gujrat) so if he submits codes you also can see notification with the solution response that your recent submition is successed or failded, so multiple websocket servers can subscribe to the same event and ------>
// ------> then this single event goes to  person in maharastra and the person in gujrat as well, but why both of them need, only one person is in marhstra on dashbord and one in gujrat are using the same account so notification should goes to both of them, this is a breaf fo the publisher subscriber system where in this case worker publishs to pubsub and whoever want, can subscribe to pubsub, if had single websocket server layer then the worker can directly to websocket server and since we have fleet(lot of) of them we want to go with pubsub

// now lets learn the final architecture(see pic 14) and understand final thing - 
// worker takes task from queue senquncally and when they find a final solution(whether it is ac or tle) they tell pubsub that user id 1 done somting,  whichever websocker server layer user 1 has connected to recives via pubsub and sends it back to a browser that is the bref of the architecture today and that is where you need messaging queues and pubsubs

// uglyest version of litcode (see pic 15 16) with this they can explore your system with some system command and get your system info may your envs then what is best approch, see this is your primary backed you do not want to run maliciouts code here and want it ro run on diff ec2 machine but the problem is how do you know how many ec2 machine you should have shuld you have 1 ec2 machine what if 20 people sends it at the same time how single ec2 machine handle 20 submitions it can not, which is why you maintain queue and put every reauest goes to quesue  and they are one by one picked by my worker and this worker is on seperate machine if user code is like system('ls') so now this is fine because that will run worker machine there is not are acture logic and that is empty machine or workers, there job is to process the code so whatever will happen it will happen in one single worker but your other all worker will be processing there requests (and if user sends some code with infinite loop the you have some check to TLE time limit excided so your worker will auto matically terminate that process after desired time limt(10s), if 20 people comes and for processing you have only single machine still you can handle 20 request 1 by 1 with the help of queue (see pic 18) and if this queue became really long then you should scale up the number of worker you have 

// now once the worker finish procesing user reauest then ugliy solution is your worker put entry in db and your client keep polling the your primry server at where your node js process keep polling the db for status is it done, is it done like that (see pic 20) and this is what letcode actully does this is ease and this approach does not need pubsub, now why do you need pub sub - if you have fleat(multiple) of websocket servers that you use to push event to the client, then your user rendomly connetes to a websocket server and when it does it(websocket server) can tell pubsub system i have user id 1 if you have any info releted to userid 1 tell it to me so i forword it to him (this is real time feed) and your worker also puts entry in db but also tells pubsub that user id 1 just done submition done, if there is time limt exceeedd please let him know then that event reach websocket 2 via pubsub and another reason is for having pubsub is what if thre is another feture in letcode that whoever is live on lit code there will be rendomly bounty here, if you actively live on litcode, you will rendomly see bounty here clam 5 dollor and you can click on it, let say lit code introduces this(if you active on lit code you rendomly get revord) (see pic 22)if thay want to add feature like this all they have to do is, they can publish to this pubsub that irrespective of user id: any send the lottery_button who are currently connected to websockt layer(have persistant connection), any you can say that all the websocket server are all subscribed  to event user: any, there is an event that comes form user:any it will reach all of the websocket server( it is lotery use case that we will see in some else video ) here we just show how you can have multiple publishers also and multiple subscribers also, but right now lets keep it simple worker tell pubsub, and if there is not any user active then worker publish to pubsub any no one will pick it up, and what if after some time that user become active then they can simplay refetch that event data directly from db, now no need to send via pubsub that is how you do message queues and pubsubs in leet code, the first set of acrchitecture remain same fi you are building youtube video trnscoding system and second part also remain constant if you have pubsub like implemetation even for chat also pubsub is good way to scale

// now let's leran redis, what is redis (see pic 23,24)
// 1) in-momery data structure store
// 2) used as database -> even it allows you to store the data but you should not used it as your primary database, why? -> so watch this 20 min vide0 link(see pic 25) it show if you want to use as primary db then you can but it will make your life very complecated, if you can not use it as db then why they introduce as db because radis is aggresively used as cache the data(what is cacheing mean see pic 26) assume you have your artical website where thre are some posts that you update one in week, now assuem when request comes form frontend you send to backend it feach the data form db and sends back to client what if 1000 request comes togenter then you sending 1000 db calles to featch the data how offen you think this pages changes one in week right, if it does not changes offen then you really need to hit this db(someone says static site generation static side generation for next js this is for node js backend), here the aswer is cacheing, you want to cache the data, you can cache the data in inmomery in your node js process by createing variable array and any request for next 10 min you dirctly serve from here from node js variable that is one thing you can do but here is problem when you start worker process then this cashed data is avaleble for only perticular process only beacause each process does have it's own memore they do not share memory though there are way but not efficient,(see pic 26) that why we cahe that data in redis so if i have 10 workers all those 10 workers takes data from redis, but 1 more thing is without redis i am hiting my db, and now first request goes to db then everytime every subsequest requst hiting redis then what the diff redis bring, redis proves you inmemorey storage, it is ruunig a process very similar to node js process where your data is stored in inmemorey so it is very fast retrival compare to your primary store(postgress or mongodb) that way usally, very offern you will see redis as caching layer rather then caching in node memorey you should in redis, and if you have more then worker and when you use redis with it for caching it is called distributed caching so that is most popular usecase of radis that it has to cache data that comes from the db to minimize the no of request that goes to the db
// 3) cache
// 4) message brocker

// you cache data in redis for two resons
// 1) you do not have to hit your db agina and again
// 2) you are doing very expensive opration

// example of how redis runs on your machine(see pic 27), as i said it is inmemory data store,(redis process is runing on ec2 server) if it is inmemory it is similar to the node js process runing on ec2 so what if your redis process dies the data was inmemore then data got lost(same thing happen with your node js process) (inmemory means it is in process if process dies data get lost and you have ot hit db again at least once to cache it again), so what if redis goes down, assume that redis goes down for some reson then it does not matter because it is cached data it is not your primary data your primary store is still(mongo or postgress) so will again start redis again and recache data again and we good to go and when it is down you have to hit the database more but that is fine, but if redis goes down and though it is inmemore db still it recover back to it previous state by doing two things, two jargons to introduce very commonly used jargons in a system like this where you need somting in inmemomery but you also want to recover it if it goes down how can you do this (another example of this whenever you goes to backpack.exchange and place and order let say click on buy(click on buy or sell) this reauest goes to a queue and form this queue it get pick up and it store in a in memory order book(what is order book see pic 28) whenever someone placed oreder that i will sell 1 usdt for 1.00003 and some one is placed order i will sell for 1.00004 traders places these orders all of these orders also in inmemorey in rust process they are not stored in db theay are in momery that mean that are litrely in variable called orderbook = [] beacause this need to be veryfast the order placement need to be very fast so here also what if the rust process goes down in your order book was in inmemory all the data get lost so here also the same architecture is uesd same as redis  to make sure if the inmemory thing goes down you can recover the state the exact order book before it went down event though you are storeing in inmomery you can do it there are two ways, way no 1 is you maintan queue of eveny evnet that someone placed event for trade on  and order to buy this much and sell for this much since the begining  all the order that have been placed you maintain them in a queue(see pic 30) and if it ever goes down you just replay these everts form queue and store them in a queue, redis also lets you do same thing), redis lets you store data in two ways(see pic 29) ---->
// -------> either AOF(append only file) which means it will keep upending that you add this data, you remove this data in a very long file and then whenever the redis process goes down if you have to bring backup you will take that file    run all the events from top to bottom and we will be able to recover the state that is 1. here you litrally have record of all the events scince the begining if you exchange for 1 year then you have record of every order that is placed you can replay that queue of messsges and recover your inmemorybook , you have sow the problem with this approach that queue becomes a very long if this is going out for a year, and whenever order book goes down it will takes 20 mins to order book to comeup because it replay all the milon, billion events that are been happened here then how can you do somting better, how can you make sure that how can i fast recover my orderbook it would have be nice, and we can do it by somting called snap shorting you can snapen short the currecnt order book every one hour, hour no 1 of our exchange looks like this hour no 2 likes like this(see pic) so you have snapshorts every hour and you have the hole queue , you can take a latest snapshort + all the events came after form queue and recover the order book that is another way you can recover an inmomery variable and redis(see pic 29,32) does the same thing you can do redis database file (point in time snapshorts) so here in this 2 way or method you can tell the redis configration(see pic 29,32) save 900 1 #save dataset every 900 seconds if at least one key is changed, what is 1 key has changed mean some single thing changed in queue -> (someone put some data or remove some data),  (see pic 27)if one key has changed in last 900 seconds then take a snapshort so you have snapshort of redis at every point and you can recover the data from here, this comes in interview to build the inmemory data structure then this you can exaplin adn this architecture is inspired form redis, if you do not understand this no buddy want to know the internals of redis

// instend redis for inmemory you can use hash maps but in mutliworker env it become similer to node process variable so redis is better

// once the snapshort is taken can we clear the queue you can clear the queue if you want but redis do not do it maintaines called upendonly logs for forever if you enable upendonly log it will maintain log forever code be wrong but check on internet same thing for postgrees also prosgress has a concenpt called write a head file (WAF ) it also pretty much does a same thing, every evert that reaches to actural postgres db it get writtin in the file

// Note : you have cache data for 10 min what if the database value changes then plople will see stale data or old data for 10 min so usally what happens is  whenever there is write opration mean admin comes and send the request taht add this new artical, whenever there is write event you clear redis as well you send the request to redis and clear the specific thing so now whenever someone ask for data they get fresh data from db. one more question is if thre is get req you cache them in redis but what if thre are two browser one is user browser one is admin browser that is adding new articales to backend, now if your user borwser get data form db and now it is cached for 10min, now at 3rd min admin update the data then which opraion(opration see pick 34) will you do out of 1,2, and 3, choice 1 is the right answer, why 2nd option was wrong it might happen you have updated data in redis and your server went down then your postgress does not have lates data so after 10 mins your redis cache data is also gone and if your server goes up after 6 second then your users will see first letest data then old data  in step 3 it update in postgres and your backed server down so you can not gerenty that both will happen(in this case even though admin updated new data if your server goes up after 6 second for 4 seconds your user will see old data even though data is updated that is why 1 is right answer) your backend server is very unpredictable, in first case 

// does not starting ec2 machine takes time do we use aws lambda to mitigate this or someting else so it does take time but there are several options here 1) you do not everytime user ec2 machine you have cuber natic cluster where you bring up more ports and bring a ports or docker containers is much faster then bringing up this thing(if you have 100 machine if peoples comes and start submiting thre code you start docker containers) (we will lean in future) , but short answer is we do not start new ec2 instance you have cluster this is autoscalling cluseter where you have  50-100 macine based on current load on queber natic

// inmemoroy - mean it is in ram

// Note : for this redis or rebit mq you shuld have diff server or ec2 machine

// https://medium.com/@deepakchandh/different-types-of-redis-architecture-3181950b68b7

// Note : whenever you run postgress in your machine locally or via docker you can use psql cli in terminal and pass postgres command so while working with redis you need to install redis cli but because we are using some docker image it is already intall over there so you just go inside the redis container and run cmd redis-cli and talk to redis you do not have to instaill in your machine, can you install in your machine yes you can but it is not needed

// run redis locally with docker (see pic 33) 
// 1) run redis container
// 2) go inside your containerr to access redis cli
// 3) once you inside the redis container run cmd redis-cli to go inside the cli mode

// redis also used as massage brocker(see pic 23) mean it mean tow things that we disscuss earlier meassageing quesues and pubsub that is want we focus on today we also use redis as cache and db as well but first we focus on these two above asspects

//  it works to store data and work as message brocker

// let first store the data (see pic 36), it is like storeing key value pare
// cmd SET key "values"
// cmd SET urvish "sojirtra"

// if you are using redis to store as cache (unfortunatedly we can here store only string not object but we can store object as string so we first stringify it and can store it (see pic 37)) this is how i would cache the data then how i would get the data like this (see pic 38)

// to get the stored data
// cmd GET urvish

// to delete data after delete i access it get nil mean noting exitsts
// cmd DEL urvish

// here one more thing to understand is hash set and hash get - if you want to store multiple things for single key like username this this, email id is this then use HSET - if you want to store the data more in sql fation
// HSET(hash set) user:1000(user with id 100) name "this"(name is this) and so on.... in future may be have address also

// to get the data of that pericualr user (see pic 40)
// HGET user:100 name

// if you want to store complex data use HSET,if you wan to store simgple data use SET 

// but if you want to store user specific for role specific data the use so if you want like if there is paid users then they should get data like that then do somting like this (see pic 41) store the data user with user id notheing elSE

// we can do this above caching useing node js also but our focus is on queue and pubsub also do it with cli and with howt do it with node js

// queues using cli and node js and let's build the system like this(see pic 42) where browser sends a reauest to primary backend backend puts it in a quesue and worker pick it from the queue wo we have to right primary backcode, worker code we do not need browser we send request form postman but we have to right two node js process a primary backend that pushes to redis queue and the node js worker that pulls from the redis queue  and  process a queue 

//(see pic 43) you can also push to a topic/queue on redis and other process can pop it e.g good example of this is leetcode submitions that need to be proceessed asynchronouly

//(see pic 44) the way to push in queue in radis is this with cli

// LPUSH <queue name - problems> <userdata - 1 we do not generayly put 1 we pull all the data that we want to process may json data object or araray by json.strigified string with multiple object and node js process will parse it to json leter on>
// LPUSH - mean push from left side

// Note - if you are pushing form left then pop form the right otherwise it will became stack(see pic 45)

// to pop from queue
// RPOP problems

// and similar to this there is RPUSH and LPOP

// blocking pop mean i will wait for certain seconds untill someone comes and push somting i will quickly pop it and exit

// BRPOP problems <0 - means wait for infinite time utill someone comes and push somting pop and exit>
// BRPOP problems <5 - means wait for 5 seconds if someone comes during that time pusj somting then pop and exit or if time elaps without gating any push then just exits after 5 seconds>

// check above senario by creating two redis client in terminal whather it works or not in one therminal do BRPOP and it will block the main thread and resolve it when someone push to queue (see pic 46)

// now lets implement the first part using node.js (see pic 43,47,48,49) the second part pubsub will see letter on, with node we use the redis library, will help use to talk to redis database

// creaet empty node js folder
// inside that create two folder
// 1) express-server
// 2) worker

// in both ther folder run these command to init packege.json
// npm init -y
// npx  tsc --init

// in both the folders in tsconfig.josn change root dir to ./src and outdir to ./dist
// create src/index.ts folder in both the folders

// what worker need to do it just need to pull somting from redis and do a fack process on it we are not running the code to run end user's code  that we will do once we reach to cuber natic her we just take the data from queue and then run fack timer that it is processing the code and then sending back acceted to user   so in worker folder just instal redis as dependiency

// but primary server need to do two thing expose the http endpoint means it need express and also need to push to a queue which means it need redis on the express backed install two thing redis and express

// in express-backend import createClient from redis this allows you to create a client and on this client you can use lpush or lpop and one more thing you have to do is await client.connect() 

// note : both the code i wrote in respective file so check there

// what this workers do, when you start these workers, they are just polling the queue do you have somting, do you have somting, do you have somting okay you have somting okay give it to me i will run it and then do things afterwords send to pubsubs, but they need to infinightly polling the queue so that we pull for loop (check the worker code file)

//async function main() {
//    await client.connect();
//    while (1) {
//      const response = await client.brPop("submissons", 0); // Note : instead of running this while loop infinite with rpop(it always sends null time to time and your loop will keep running) use brpop and await it so you get blocked here untill you do not get something then process submition
      // here you can run users code in decker container with docker exec
    //   console.log(response);
    //   await new Promise((resolve) => setTimeout(resolve, 1000));// this is where do that expensive opration
      // send it to pubsub
    //   console.log("Processed users submissions");
    // }
//   }
  

// after this run worker code 4 time with diff terminal window and start 4 clients because in real world you have multiple workers beacause your website have 100 of users, (see pic 50) i have started 4 workers and all 4 them are waiteing and ( blocked) for when someting will comes in the queue now lets put somint in queue with express backend and i will start sending postman request, (see pic 51) as i add data in queue it is picked by rendom workers , what if all the workes are down(stop all 4 codes), that is fine we will keep piling up in queue (so it wait inside queue), and as you start your worker(workers goes up) it will again start processing your queue, this is the banifit of async system if the worker are ever down you just bring them back and they again start polling from the queue and this is how you can build the first part of leet code(see pic 42)

// the thing which is remain(see pic 52 this we cover in letter ofline video) is what is pub sub and how woker can publish to pubsub to websockt server

// Note : queue usecase is important if you are bulding the system like leetcode because in leet code you are letting the end users run an expensive opration on your compute(your machines) and when you do that make sure that you do not run that in your primary backend and also make sure you have limited workers, 3 workers, if lot of people send me data i can tell to workors that do this, do this, do this, i can just push it in queue and hope that workers will auto scale eventuly if the queue is long then they become 20 workers and pick up and start to process that is why one usecase of using message queues

// Pubsubs - what if a worker that have processed the user's code what if they want to send to end user that here is your submition accepted or rejected how they do that the worker can tell directly to the browser, wo workes can publish to pubsub that is runing on ec2 that can tell the user via pubsub (that this is user 1 with id 1 that and this is his code status) to websocket server to user browser 

// redis run on single threse so it can not run two pop opration at time it will do one by one

// question - what if when you pop soming from queue and the your posecesing server goes down and can process your request but task alredy pop from queue here you can use, use redis queue acknowlagement if i pop somting form queue and if i do not respond back in some time then  queue assume it did not preceess, again push that item in queue, 

// AWS SQs only provides you queue but redis provides you more

// sns simple notificaiton servicex