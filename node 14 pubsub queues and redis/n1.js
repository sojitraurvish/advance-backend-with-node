
// websockets are not that much used for backend to backend communication they are really user for forntend to backend communication

// and these to are mostly use for backend to backend communication, it will help two diff backend to each other let dive deep in pubsub and messaging queues  

// today we will dive deep in leet code example (see pic 1)
// 1 ) queues
// 2 ) pub sub
// 3 ) redis

// we will need docker to start radis or else you can use avin to use redis instance online(but docker option is good it gives you bunch of utilities)

// why we learingin about radis beacuse redis give you a bunch of things, and two of that things are pubsub and queues, then can i do pubsub without redis, yes you can do there are many library, packages, and implementations that let you do pub subs (rabitmq is good example of queues) (sms is good emample of pubsub not technically and it is service of aws) and there are verious implementation of pubsub and redis is one of them that let you do pubsub, what are pubsub?

// now lets understand in a system like lit code where do you need queues and where do you need pubsub , the final achitecture is bit complex(with queue and pubsub) (see pic 2) but we get there soon but let look at fist example with just a queue(see pic 3) the architecture says anytime a user is on your plateform, so supporse the user is on let code and try to submit the problem the req would goes to the primary backend or yours(see pic 3), what is the primary backend user simple express server recieve the request from the browser that problem id 1 or  in this case problem id two-sub(see pic 4), code is some code snipit and the language so this input reaches the primary backend and this pushes this into queue, why it is pushes into queue can we execute the code on primary server here only and tell them the response that your code is succeed or failed, the reason why you deligate this out in  queue is that what if the user sends you the code that is malicious it may give you infinite loop so your main server may get bussy by processing single request (which should also server request to other users), and that why we deligate code to some other places and that why we have worker nodes that will process your request(why it is called workder nodes beacause there job is to pick things work on it submit that work and pick another work), so primary backed puts those taskes into queue and your worker picks the problem form queue, but why we need queue, can not my primary backend directly tell worker nodes that do this job and that why i do not have any sequrity issue on my primary server the reason you can not do that is 100 people sends you the request if 100 people sends you the code that you do not want to send up same worker(W1 - which has only 4gb space), you want to garenty, whenever you are only litcode, litcode geneties that your code will run on 2core machine with this much cpu and all, you have to make sure that garenties are mit and you have make sure every worker run single code at time and queues become very popular usecase in this case, even if you have only two workers and 20 people submits the problem you will have long queue and worker will pick slowly, a lot of people may be waiting for response but you can garenty that worker one pic 1 process at time, worker 2 pick 1 process at time, it will run it store the response somewhere and then it will pick the next one, so to make sure you pick the things one by one -------->
// ----------------> and to make sure your if primary backend is overwhelemed if everyone is trying to do a very expensive opration very fast the you can push that opration in queue and slowly we will do process that with one of the workers, the another banifit of queue is if you have 0 workers still pleople sumitions stays in queue and whenever you bring a worker it will start pickuping from queue, another benifit is you can auto scale your workers you can say like if your queue length become 100 i should start new empty worker and if queue length become 3 then i should stop some of them and say only one worker will stay and handle all the request (so you can auto scale your workers up and down), this is become supper important if on litcode there are 100 people on your platefrom you do not want to alive 100 worker at time because these are very expensive machines so you need to make sure you need to auto scale them very aggresively and here queue is good use case and that is the one usecase of queue where you might want to use queues, generally whenever you have an expensive operation that need to run on single machine, another example of this video transcoding(what is video transcoding - if i go to youtube and upload a new video it takes some time video to processes i upload a single 10mb video that video get converted into 4 - 5 diff qualities(see pic 5) this is called video transcodding this is also very expensive opration here also you want that whenever user uploads video on youtube it goes into a queue then workers eventually pick up and transcode the video ) so that is another use case where queue makes lot more sance

// the good example of queue is rabit mq, sqs simple queue in aws, or redis also let's you do queues 

// Note - whenever user want to do expensive opration on your machine a log runing expensive opration(submiting problem like lit code or upload video for transcodding) you probebally wnat to use architecture like this(see pic 3) queue and you want to upscale and downscale the workers based on the length of the queue that is when you need queues

// and when you need pubsub or publisher subscriber even before that lets learn what is pubsubs

// pubsub - let look back to our pevious example (see pic 3) user submit some code, sends it to primary backend, it sends it to queue and from there it get pick up by one of the worker process, once it picked up the worker need to tell the browser that you have accepte or rejected but it need to send the final result to the browser if remember from yesterday how does letcode does it lit code ueses polling (see pic 7), once i submit the code or submit request it start to send check request again and again, poll the serve again and again is the submition(result) done and get pending state(see pic 9) and right now it falild for this specific error(see pic 9) so let code uses polling but can you do someting better can you use websockets that we learn in last lecture, but what is the benifit of using websocket here, if use websockets then your browser won't constantly poll the backend like (is is happen...,is is happen...,is is happen...,is is happen..., and so on untill not finished) it won't overwalmed your primary backend and which will internally overwalmed your database because every step you are writing to your db as well, whenever you make submition entry goes to db and says this problem is processing, whenver worker done it update entry that it is done or rejcted but when the user long polling then it is overwheming backend again and again, but what if a server(worker where code is processing) could push that bro i am done and tell browser so it can show to user for pushing event from server to client what we use we discuess yester day we use websockets so want can happen after the worker is done, after worker is done processing your code, getting response, checking if it is correct it can send the response to websocket server(see pic 10) which is connected to a browser, whenever i goes to litcode what happen if there is persistant connection to websocket server (if i am on this page see pic 11) what if along with http request that iam sending there pesitsnt websocket connection to websocket server anytime i click one run the worker finally process it(see pic 10) the worker can singnal to the websocket layer  please tell user with id 1 if he connected to you if user with id 1 seleted to you tell them that the status of the recent submition is TLE(time limt excied) that is how worker can talk to websocket server and publish event to browser, in (pic 9) you can see that between worker and  websocket server we used pub/sub but why we used that -------->
//  why can not worker directly tell websocket server, why can not worker directly talk to browser, why we are complicating this flow, 1) worker can not directly connect to browser worker are very transitive, they come up, goes down, they should never be expose to internet , the worker job is that bro give me code i will run, put the entry in database and jayada se jayada i will publish to pubsub i will not do anything more, (and these worker very frequetly goes up and down that why you do not want your end user to connect it with directly),  so you have fresh service(mean node js application) that browser connect to, wo your browser connects to this websocket layer and whenever the worker completes the submition it can tell pubsub that who one with user id 1 please send them this specific status(see pic 9) for this submition,  but why we need pubsub why our worker can not directly tell websocket layer via http but why we need here pubsub and answer already discussed at the end of the yesterday class --answer-->(see pic 12) that in real word you have multiple websocket server not only one,(one websocket server can support only 10,000 request), so you have multiple websocket server and your user connected to one ws server out of all of them, let say user1 has persistant connection to ws3, so whenever your worker is done it does not know that should i send this information to ws server 1, ws2, or ws3, instead that your worker publish a event to pubsub and whenever user connects to websocket layer(whenever there is conection between user id 1 with websocket server3 ) (user1 throw ws3)it can subscribe to an event for user id 1(event called may be userid 1 ) in pubsub and worker can publish if worker know there is submition for userid 1, that way this worker directly reach to websocket layer (vs that fugering out that are you the ws server which is connected to user 1), but with pubsub our worker says i will publish to pubsub whoever(websocket server) wants it please subscribe to pubsub,(see pic) it might happen that you have two browser tab open or you have mobile app open and the mobile app is connected to ws1 server(you sared letcode acc to your friend in maharastra and you are using web in gujrat) so if he submits codes you also can see notification with the solution response that your recent submition is successed or failded, so multiple websocket servers can subscribe to the same event and ------>
// ------> then this single event goes to  person in maharastra and the person in gujrat as well, but why both of them need, only one person is in marhstra on dashbord and one in gujrat are using the same account so notification should goes to both of them, this is a breaf fo the publisher subscriber system where in this case worker publishs to pubsub and whoever want, can subscribe to pubsub, if had single websocket server layer then the worker can directly to websocket server and since we have fleet(lot of) of them we want to go with pubsub

// now lets learn the final architecture(see pic 14) and understand final thing - 
// worker takes task from queue senquncally and when they find a final solution(whether it is ac or tle) they tell pubsub that user id 1 done somting,  whichever websocker server layer user 1 has connected to recives via pubsub and sends it back to a browser that is the bref of the architecture today and that is where you need messaging queues and pubsubs

// uglyest version of litcode (see pic 15 16) with this they can explore your system with some system command and get your system info may your envs then what is best approch, see this is your primary backed you do not want to run maliciouts code here and want it ro run on diff ec2 machine but the problem is how do you know how many ec2 machine you should have shuld you have 1 ec2 machine what if 20 people sends it at the same time how single ec2 machine handle 20 submitions it can not, which is why you maintain queue and put every reauest goes to quesue  and they are one by one picked by my worker and this worker is on seperate machine if user code is like system('ls') so now this is fine because that will run worker machine there is not are acture logic and that is empty machine or workers, there job is to process the code so whatever will happen it will happen in one single worker but your other all worker will be processing there requests (and if user sends some code with infinite loop the you have some check to TLE time limit excided so your worker will auto matically terminate that process after desired time limt(10s), if 20 people comes and for processing you have only single machine still you can handle 20 request 1 by 1 with the help of queue (see pic 18) and if this queue became really long then you should scale up the number of worker you have 

// now once the worker finish procesing user reauest then ugliy solution is your worker put entry in db and your client keep polling the your primry server at where your node js process keep polling the db for status is it done, is it done like that (see pic 20) and this is what letcode actully does this is ease and this approach does not need pubsub, now why do you need pub sub - if you have fleat(multiple) of websocket servers that you use to push event to the client, then your user rendomly connetes to a websocket server and when it does it(websocket server) can tell pubsub system i have user id 1 if you have any info releted to userid 1 tell it to me so i forword it to him (this is real time feed) and your worker also puts entry in db but also tells pubsub that user id 1 just done submition done, if there is time limt exceeedd please let him know then that event reach websocket 2 via pubsub and another reason is for having pubsub is what if thre is another feture in letcode that whoever is live on lit code there will be rendomly bounty here, if you actively live on litcode, you will rendomly see bounty here clam 5 dollor and you can click on it, let say lit code introduces this(if you active on lit code you rendomly get revord) (see pic 22)if thay want to add feature like this all they have to do is, they can publish to this pubsub that irrespective of user id: any send the lottery_button who are currently connected to websockt layer(have persistant connection), any you can say that all the websocket server are all subscribed  to event user: any, there is an event that comes form user:any it will reach all of the websocket server( it is lotery use case that we will see in some else video ) here we just show how you can have multiple publishers also and multiple subscribers also, but right now lets keep it simple worker tell pubsub, and if there is not any user active then worker publish to pubsub any no one will pick it up, and what if after some time that user become active then they can simplay refetch that event data directly from db, now no need to send via pubsub that is how you do message queues and pubsubs in leet code, the first set of acrchitecture remain same fi you are building youtube video trnscoding system and second part also remain constant if you have pubsub like implemetation even for chat also pubsub is good way to scale

// now let's leran redis, what is redis (see pic 23,24)
// 1) in-momery data structure store
// 2) used as database -> even it allows you to store the data but you should not used it as your primary database, why? -> so watch this 20 min vide0 link(see pic 25) it show if you want to use as primary db then you can but it will make your life very complecated, if you can not use it as db then why they introduce as db because radis is aggresively used as cache the data(what is cacheing mean see pic 26) assume you have your artical website where thre are some posts that you update one in week, now assuem when request comes form frontend you send to backend it feach the data form db and sends back to client what if 1000 request comes togenter then you sending 1000 db calles to featch the data how offen you think this pages changes one in week right, if it does not changes offen then you really need to hit this db(someone says static site generation static side generation for next js this is for node js backend), here the aswer is cacheing, you want to cache the data, you can cache the data in inmomery in your node js process by createing variable array and any request for next 10 min you dirctly serve from here from node js variable that is one thing you can do but here is problem when you start worker process then this cashed data is avaleble for only perticular process only beacause each process does have it's own memore they do not share memory though there are way but not efficient,(see pic 26) that why we cahe that data in redis so if i have 10 workers all those 10 workers takes data from redis, but 1 more thing is without redis i am hiting my db, and now first request goes to db then everytime every subsequest requst hiting redis then what the diff redis bring, redis proves you inmemorey storage, it is ruunig a process very similar to node js process where your data is stored in inmemorey so it is very fast retrival compare to your primary store(postgress or mongodb) that way usally, very offern you will see redis as caching layer rather then caching in node memorey you should in redis, and if you have more then worker and when you use redis with it for caching it is called distributed caching so that is most popular usecase of radis that it has to cache data that comes from the db to minimize the no of request that goes to the db
// 3) cache
// 4) message brocker

// you cache data in redis for two resons
// 1) you do not have to hit your db agina and again
// 2) you are doing very expensive opration

// example of how redis runs on your machine(see pic 27), as i said it is inmemory data store,(redis process is runing on ec2 server) if it is inmemory it is similar to the node js process runing on ec2 so what if your redis process dies the data was inmemore then data got lost(same thing happen with your node js process) (inmemory means it is in process if process dies data get lost and you have ot hit db again at least once to cache it again), so what if redis goes down, assume that redis goes down for some reson then it does not matter because it is cached data it is not your primary data your primary store is still(mongo or postgress) so will again start redis again and recache data again and we good to go and when it is down you have to hit the database more but that is fine, but if redis goes down and though it is inmemore db still it recover back to it previous state by doing two things, two jargons to introduce very commonly used jargons in a system like this where you need somting in inmemomery but you also want to recover it if it goes down how can you do this (another example of this whenever you goes to backpack.exchange and place and order let say click on buy(click on buy or sell) this reauest goes to a queue and form this queue it get pick up and it store in a in memory order book(what is order book see pic 28) whenever someone placed oreder that i will sell 1 usdt for 1.00003 and some one is placed order i will sell for 1.00004 traders places these orders all of these orders also in inmemorey in rust process they are not stored in db theay are in momery that mean that are litrely in variable called orderbook = [] beacause this need to be veryfast the order placement need to be very fast so here also what if the rust process goes down in your order book was in inmemory all the data get lost so here also the same architecture is uesd same as redis  to make sure if the inmemory thing goes down you can recover the state the exact order book before it went down event though you are storeing in inmomery you can do it there are two ways, way no 1 is you maintan queue of eveny evnet that someone placed event for trade on  and order to buy this much and sell for this much since the begining  all the order that have been placed you maintain them in a queue(see pic 30) and if it ever goes down you just replay these everts form queue and store them in a queue, redis also lets you do same thing), redis lets you store data in two ways(see pic 29) ---->
// -------> either AOF(append only file) which means it will keep upending that you add this data, you remove this data in a very long file and then whenever the redis process goes down if you have to bring backup you will take that file    run all the events from top to bottom and we will be able to recover the state that is 1. here you litrally have record of all the events scince the begining if you exchange for 1 year then you have record of every order that is placed you can replay that queue of messsges and recover your inmemorybook , you have sow the problem with this approach that queue becomes a very long if this is going out for a year, and whenever order book goes down it will takes 20 mins to order book to comeup because it replay all the milon, billion events that are been happened here then how can you do somting better, how can you make sure that how can i fast recover my orderbook it would have be nice, and we can do it by somting called snap shorting you can snapen short the currecnt order book every one hour, hour no 1 of our exchange looks like this hour no 2 likes like this(see pic) so you have snapshorts every hour and you have the hole queue , you can take a latest snapshort + all the events came after form queue and recover the order book that is another way you can recover an inmomery variable and redis(see pic 29,32) does the same thing you can do redis database file (point in time snapshorts) so here in this 2 way or method you can tell the redis configration(see pic 29,32) save 900 1 #save dataset every 900 seconds if at least one key is changed, what is 1 key has changed mean some single thing changed in queue -> (someone put some data or remove some data),  (see pic 27)if one key has changed in last 900 seconds then take a snapshort so you have snapshort of redis at every point and you can recover the data from here, this comes in interview to build the inmemory data structure then this you can exaplin adn this architecture is inspired form redis, if you do not understand this no buddy want to know the internals of redis

// instend redis for inmemory you can use hash maps but in mutliworker env it become similer to node process variable so redis is better

// once the snapshort is taken can we clear the queue you can clear the queue if you want but redis do not do it maintaines called upendonly logs for forever if you enable upendonly log it will maintain log forever code be wrong but check on internet same thing for postgrees also prosgress has a concenpt called write a head file (WAF ) it also pretty much does a same thing, every evert that reaches to actural postgres db it get writtin in the file

// Note : you have cache data for 10 min what if the database value changes then plople will see stale data or old data for 10 min so usally what happens is  whenever there is write opration mean admin comes and send the request taht add this new artical, whenever there is write event you clear redis as well you send the request to redis and clear the specific thing so now whenever someone ask for data they get fresh data from db. one more question is if thre is get req you cache them in redis but what if thre are two browser one is user browser one is admin browser that is adding new articales to backend, now if your user borwser get data form db and now it is cached for 10min, now at 3rd min admin update the data then which opraion(opration see pick 34) will you do out of 1,2, and 3, choice 1 is the right answer, why 2nd option was wrong it might happen you have updated data in redis and your server went down then your postgress does not have lates data so after 10 mins your redis cache data is also gone and if your server goes up after 6 second then your users will see first letest data then old data  in step 3 it update in postgres and your backed server down so you can not gerenty that both will happen(in this case even though admin updated new data if your server goes up after 6 second for 4 seconds your user will see old data even though data is updated that is why 1 is right answer) your backend server is very unpredictable, in first case 

// does not starting ec2 machine takes time do we use aws lambda to mitigate this or someting else so it does take time but there are several options here 1) you do not everytime user ec2 machine you have cuber natic cluster where you bring up more ports and bring a ports or docker containers is much faster then bringing up this thing(if you have 100 machine if peoples comes and start submiting thre code you start docker containers) (we will lean in future) , but short answer is we do not start new ec2 instance you have cluster this is autoscalling cluseter where you have  50-100 macine based on current load on queber natic

// inmemoroy - mean it is in ram

// Note : for this redis or rebit mq you shuld have diff server or ec2 machine

// https://medium.com/@deepakchandh/different-types-of-redis-architecture-3181950b68b7

// run redis locally with docker (see pic 33)