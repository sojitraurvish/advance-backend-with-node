// what is vertical scaling mean you increase machine size and with  cluster module create as many worker process you want with as per cpu cores, but after some point you can increase the size of your machine then you have to goes to vertical scaling

// there is limit that how much you can vertically scale 
// you can get as much bigget machine you want but here some problem you have very big machine and everyone is sending request to the same machine then it is a problematic 
// 1) you are being dependent on a single machine if machine goes down you have single point failier
// 2) you are telling all billion user to connect to same server what if some one in us someone in india your machine can only be in us and hance indian people have lot of latency so idially you should horizontally scale 

// what is horizontally scallin mean (see pic 3)
// that you should have bunch of servers not just one  each server is very powerful that is fine each server can have node js running in cluster module that is fine as well, there should be multiple server and when user goes to your website the requset should first goes to load balancer which you know machine in it self which as name suggest(balances the load) so it send  the request to one of many servers that is how you horizontally scale, why it is called horizontally scaling - because theorotically you can say i am adding more machines horizontally that why

// will see how to scale horizontally by some aws constracts something called auto scalling group, the easyest way to do auto scaling is i will have 1 machine innitially but as the load goes above 50% on this machine i will add another machine and now load should be probebally bye 25% because i doubled the number of servers then let say the more traffic came and load again become 50% (what do you mean by load - cup usges) then i scale again one more time and net load on system is 33% if again it goes above 50% i will scale up more and if it want down to 25% then we decrease the no of servers this is the easiest way to auto scale based on either incomming bandwidth, or incomming request count, or avg cpu usage of your cluster so these are easier ways to scale where you do not have any ade hock logic - mean somting like these architecture(see pic 1)  for example yesterday we discuss if we have realtime game like chess, then you have bunch of machine let say this is (see pic 1 S1) aws machine which has 6 cores running in cluster module so i have 6 process running in S1, 6 in S2, 6 in S3, 6 in S4 and all of these has persistent websocket connction ot the user, and let say you know in node js if you are creating websocket connections, 10,000 is good no if you have more than 10,000 users you will start to create issues or you start ot face issues you are not dependent on cup usages you can dependent on some number that how many people are currently playing if 88,000 people are currently playing then you want 9 machines, because more than 10,000 people comes on single server it a problem, so this is ade hock scaling and there is matic that you own , the above matric () 1) incomming bandwidth 2) incomming request count 3) avg cpu usage fo your cluster) that we talked about that somthing that the clould providers automatilly get you do not have to do lot of efforts when you scaling like this you just need to tell the cloud provider in this case aws these are my rules(1,2,3 just above) if the incomming bandwith average above 50% you increse no of server and if it goes down you decrease no of servers, but (see pic 1 architecture) this is more ade hock logic and the cloud provider does not know this, there is not option to see current open connections or no req per second you can see if you are goring through load balancer or(you do not need this no cont if it si paytm like app archetecture see pic 2) ------>
// ------> you can have aggregate each server connection(see pic 2) req no and you create a matic which tell ASG in aws to increase 10 servers i now have 88,000 people might that is lot of my websocket servers can not  handle that ade hock scaling mean you own the matic based on that you scale and this is your system dependent matic, what can be another matic is youtube like plateform that transcode the video as people comes to your app and upload video in 1080p then this video should be conveted into three videos 1080p, 720p, 380p this process can take a really long time if i upload 2 hours video on youtube it can also take 2 hours sometimes, here also if people comes to your website and uploads the video then you have to starts a machine for them that are transcodding there videos if three people have uploaded the videos then you might have three servers that are transcoding there videos,here a good matic on which you should decide the no of machine should be up si queue length the length of the queue which stores the currently pending videos as people upload to your plateform you push there video to a queue like this person has uploaded the video what idially happens is you upload a video to a cloud provider you upload your official video whatever your video editor gave to you it actually goes to s3 or some object stores(youtube own by google google will never use amazon) so first from dashbord your video is get uploaded to some object store once this is done then this location of the video is get pushed into queue like there is this video id that harkirt had posted id :1 user_id:2 and video location(s3 or any object store) this is the first thing that goes into the queue so this queue is getting filled, why am i doing queue architecture, because thus system can tell directly to worker that please transcode, this worker may be bussy transcodding someone else video so you asyncly handle this task, you send it queue first and then this queue lenth keeps on incresing let say there is spike everyone start to upload then is queue lenth become 100 and this is a matic based on which you scale now ovv now 100 vides are pending  we should probebally scale up to 50 servers so at least 50% videos give transcoded in next hour and rest 50% in next to next hour so here again it is an ade hock matic where the queue length which decides the no of currently active workers so this is the third way -----> 
// ----> today we will implement both of this architecture(1 paytem, 2 chess app)  -  how would you do auto scaling both these matrix either ade hock matic (matich from our side) or a simple matic like cpu usages how you scale up and down on aws now let's do that

// ASG (see pic 3) but this is not the cleanest approach to do it there are bunch of ways to auto scale the better way to auto scale usally using containers(see pic ) you do not start machine again and again, you have some either very big machine somewhere where you start containers and as load increases you start more and more containers here container are more light weight and easyer to start compare to want we are doing right now what we are doing right now is letrally starting multiple ec2 machines , if you goes to your aws console then you can see multiple machine get added here it's very row way to auto scale this is how people auto scale until containers ware now there now container way is more light weight which mean more easyer to start(ec2 machine takes 20 to 30 s to start) so that is why this is not an best approach  but this is great approach if you want to avoid cueber natic if you want to aviod containers if you do not have containers in your company right now then the row way to auto scale is this in aws it is called auto scaling groups, but auto scaling it self requires a bunch of things you are starting a bunch of machine this machine need to have node js installed, this machine need to have your code that actully needs to excicute(js or ts code), so you have to some how create a snap short of your machine, like this is the machine that i want to bring up or bring down as my scale increases so there are some buzz words to know before we actully procide(see pic 3) and these are aws ASG specific buzz words but you will find them diff in diff cloud provider either with diff name or with the very similar name, the first buzz word to know is someting called image or amezone machine images (AMI) is snape short of machine from which you can create more machines, what are we going to do today we will take simple node.js app and the first thing we will do is we will create first ec2 machine and inside that we will install npm node and get our code form git hub then we will create a snap short of it, we create and image from this ec2 machine this is the first buzz word to know (what is a banifit of creating an image - if you have an image then you can create multiple ec2 machine from it you do not even need auto scaling group even without it, even if you just has ec2 machine from which you created and iamge now you have snapshort of that machine so you can keep recreating this machine ----->
// ------> which will have npm, node, code what ever is the current state of machine(from the machine you created an image) the file system, all the libraries that ware installed you have a snapshort of it ) and why is this useful because using this snap short we can create more ec2 machine  and this snapshort we have to give autoscaling group so that the auto scalling group knows that is how i will start my new servers as scale goes up, so that is the first thing to do, what is the second thing to know load balancer - as you have multiple machines then they will have there own ips, domain names but your end user usally sees what your user sees api.100xdevs.com what is that mean  is this req(api.100xdevs.com) need to be either handled by single machine or someting else it is not neccesory that load balancer as single machine but something needs to handle this and route the req to one of many machines that exsits based on the load and hance this thing is called load balancer , can i create load balancer my my self can i just start and ec2 machine which takes req from user and routes them to various machines based on the load of the machines yes you just do not need to use aws ec2 machine, you can start ec2 machine start some short of proxy implementaion there and that can handle incomming req and forword it to one of them and nginx will let you do that so if you want you can create your own, but usally you differ this to aws why because it is a fully manage service which mean you do not need to worry about scaling it ever, it might happen your load balancer becomes a bottle nack what if 1b users on your website they can be handle by single machine you have 1000 machines to handle a 1b request but you have a single load balancer the this will be a problem so aws thankfully spends a lot of time and effort here because they knows that this is load balancer so this will be the first entry point for all the users so they make sure LB scale up and down have machine in various regions so whenever your load increases due to whatever resons then it will be able to handle those incomming reqs that is why it is good idea that you should use load balancer from aws rather than creating your fresh machine and deploy worker, Target Group -  this is very aws specific, so this might not exits in any other cloud provider, a group of ec2 instances that are load balancer can send a req to, so whenever you start a load balancer ------>
// ------> how do you tell it that if you are recieving a request then please send it to S2 machine you tell it that you are part of this target group then you attach every machine that here to that target group, you tell that you are part of target group t1 this how aws expects you to link a load balancer to your machines, it(LB) says do not give me actual ips your machine or actual ids of your machine give me a target group the target group and the target group what will be connected to various machines it just creates an easy distinction if you have the more target group, then if we have another load balancer where also i can say you also route all the req to target group t1 , anther place where this will be useful is when you create an autoscaling group auto scaling group whenever it adds more machines it can simpally tag those machines with t1 target group so that load balancer can send the req them, we comes to this very soon but buzz word to keep in mind is target groups which let you attach two instances(machines) and whenever you start a load balancer you tell the load balancer that whenever you are recieving req on port 443 you will be forwording it to this target group. lastly, A Launch Template - a launch templte  that can be used to start new machines, you might say that is was an image was image was that only that you create an image and using this image you can start multiple machines what is a templete, the thing is image just takes a snapshort of your machine, it does not takes snapshort of ovv these are the ports are currently open in this machine it does not attach that storage in this machine was 8gb and bunch of other things for example when this machine starts this script should run node index.js should run or pm2 start index.js should run all of this things can not be the part of the image, image only is file system and current snapshort that it looks like this  it has these libraries and these folders a launch template has the image also that this is the image that we are starting or this is the image that we want to launch with otherthings and this is the sequrity group of it what is sequrity group - what all incoming req are posible this is the key pair that can acces this machine, this is the start code - so when the machine starts we need to run pm2 start index.js in this machine so and and so forth that is what a launch template is so there is big diff between image and templete, -------->
// --------------> when you start an instance by using just an image then you have to tell explecitly that this new instance i am starting  this is it's security group this is the space it need to have vs if you have lauch templete, every machine that starts with same sequrity groups same key pair that connect to it sam command that run and start node process so that is last thing

// Note: Plase make sure you get rid of all your resouces after this -> because when you create ec2 machine and then forget them but one thing able auto scaling group is you tell ASG that bro i need 10 machines and if you goes to your aws dashbord and if you stop 10 machine then it will restart them and it will chanrge you lot so plase make sure at the end you clean up fully properly, at the end go here and delete all the images form here(see pic 5), go here and delte auto scaling group from here(see pic 6) and not just stop your ec2 machine , if you just stop your ec2 machine then ASG will restart it

// now let's start to deploy an application

// so first step create image so from which other instance can start and for which you have to first create instance of your own so now step 1 is launchin an new ec2 instance(see pic 7) in which you will bring node and codebase(code base with clutser module see pic 8,9) , once you click on launch a instance you get form to fill up give it name week-22-asg-class(see pic 11), let me select ubanto as os(s p 12), instance type can by anyting does not matter t2.micro should be fine(s p 13) because does not matter with what type you create machine becasue when you are spinning up more machines you can always select more big machine so t2.micro is fine, key pair(let you connect to the aws machine whenever your machine starts somewhere one the internet you need to connect to it and for that you need some sort of password, you do not want any one other on internet to be able to connect it, this key pair is easyest way to connect with your server with ssh protocol, so from your machine you can connect to your server with ssh(see p 14)) so here ceate new key pair or select an existing one(create new give name(s p 15) and at the end you get the pem file that you need to use while connecting see pic 14 hiligheted are at where need to use pem file one more thing you have to do is you have to change the permition in this file otherwise you wan't be able to use this file to login via ssh so this is the command you have to run (see pic 17)), you can create sequrity group here(see pic 18 )if you do not it's fine, at time of auto of creating auto scaling groups you can override it. just leave defalut here allowing ssh traffic from anywhere that mean after this machine get started i can connect it via ssh, size does not matter you can change letter on so take defalut(s pi 19) and now click on lauch instance and that will start new machine, now what are the next steps i need to go to this machine install node js clone my code these are the two thing that i have to do before i goes to the image creation, so wait for this machine to start, now let connet to machine but how do i connect to it, fist copy the public dns or ip does not matter(se p 20), open the terminal in your pc  and run the cmd ssh -i <your temp file name(with location)> ubuntu@<the address>(see pic 21), now you are connected to machine via terminal that you just started, now instll node js, there are many ways(but follow this digtal ocean guid see pic 22) but plase install using nvm it is batter way

// (s p 23) this cmd install not only node but also nvm and it will also give you export that you just need to copy and past(see pic 24) and you have access somting called nvm, now you can use nvm to install node js now wirte cmd nvm install node now node js is installed now next step is cloing a repo, go to repo copy https url not ssh and then run command git clone, after cloneing go inside that folder and try runing cmd npm install and cmd npm run start and check if it is working or not, i can also test it is working or not by fist goes to security group here(s p 25) and openning port 3000 the problem is 3000 is web specific and until we did not added inbound rules(s p 26) for port 3000 so just to check twice(if you find logs in terminal then you app is working) then select security group here(s p 27) and edit the inbound rules(s p 28) right now i have only 1 rule i am allowing ssh access form anywhere, so anyone can hit  the port 22, now i add a rule that says anyone can hit the port 3000 as well because the port 3000 where the application is run check index.js app.port(3000) fucntion so add new inbound rule (see pic 29, 30) for port 3000 and now if i goes to ec2 machine and try to hit my machine on port publicDNS:3000  you can see your code is runing(see pic 31) now you app is deployed on internet but it has bunch of problems, so P1)  url is very ugaly beacause port at the end P2) is it says not secure here(see pic 32), P3) it is not auto scaled yet if lot of people hit there it is scrud so that we will fix, so now step 1 is done which is installing everything in your machine, step 2 is creating an image out of it so slecte the instance for you want to create a image > then selct actions >  go to images and templets > click on create image (see pic 34) which will create a snapshort of your instance at current point(so if at current point my instance has folder called week-22 then my image will also have it and the any machine i start from this image it will also have that ), (see pic 34,35) give image name, size 8 gb is fine if you want you can increase decrease it and click on create image, step 3 is creating auto security group for your final auto scaling group, you final auto scaling group has some inbould rules thins that you want to open up , click on securiy group and create new security group, give name, desc,(see pic 36,37) ---->
// ---->(see pic 38) add inbould rules create two rules for port 3000, and 22(for i can ssh into the final machine and see what is happening ) and here at port 3000 you can add more strick rule that only allow traffic from  load balancer to the instance but why only two ports only 22 is for us to connect and 3000 is for load balance to connect but here can make port 3000 restricted to world only open for load balancer right now we are not doeing but you can do that as well and click on create security group to create it

// recape  whenever you create auto scaling gorup it need few things 1) image (when i start the machine what should it look like you can be like image would be ubantu image you can start auto scaling group with jsut ubanto with no code in it but what do we want that when the auto scalling group starts it should start this application in it so we say no 1 i will create normal ec2 machine we added node js library here  and we create an image form this ec2 machine so that we can give this spcial image  to the auto scallin group that take this the ubantu, code is here, node js is also here so image with these three thins what we give to auto scallling groups, could we have just given the ubanto yes you can just give ubantu and in your start script of the autoscalling groups you will be like fist i install node js, then i will do git clone, then i will do npm install, then i do npm run start we can also do this if you want to avoid creating your custom image you can always just use and ubantu image and in your starter script(starter script runs when machine starts) is where you can write this all of the code or you can be like that this is my custom image this already has ubantu and code, and  node js so now i will run npm run start in my starter script why this approach is better, because now whenever machine starts it does not need to install node js you can simpally run npm run start here in starter scrpt but becore that you can add one more step here which si git pull to get letest code and then run npm run start)

// now i want to create a launch templete  what is launch templete go to launch templetes(see p 39) as the name suggets this is somting you create from which you can other instances but you might ask images also does the same thing(from image section also i can launch instance then why do i need lauch templete because if you goes To AMI to launch new instance from image i ask you bunch of things what is machine size, what is the instance type, key par, sequrity group, change the space i can specify these all in the image is where the launch templete comes to the picture) the launch templete says that (first goes to lauch temp seciton and click on lauch templete, here it says give me image,security group, space also everyting and i will cerate a launch templete and you can reuse this templete to start new instace as like image but somting more in it is spcification that you now no need to give again and again) > (see p 40 ) firt give it name > (then it ask you what image you want form this templete  you can here use row ubantu image but we have created our custom image so we use our custom image here(s p 41) now we choose and instance type(we can spcify all this when we start the instance and here as well but if you attach this lauch template then you do not need to spcify but if you specify once in auto scalling group then whenever you start auto sc.. group it knows what to start with and if you do not dpcify here then you have to specify it in auto scaling group so we are not specifying here)) then add a key pair(see p 42)(you want ssh access using key pair to machine), lastly storege we take default only(see p 43), then select security group that we created(see pic 44) in network settings, lastly once thing is this last section called advance details where you put somting called user data  this section is for what code you want to run when the machine starts that is what user data mean, so this is starter script so if you want to use pm2 with node in starter scipt(se pi 45 here pm2 alredy install via image), here we have to install pm2(s p 46, add this in your start script 47, if you install the node diff verstion then you script would change as per version (see pic 48)) now lauch the templete, but why we set up the parth(it put node or npm in a path) because when we start the machine it does not have the access to npm  because this starter script probable run before npm put in to path which is why you have to expicitly put ----->
// ----> because this starter script starts as soon as the machine starts and as soon as machine starts it does not access to this npm library or use can do this as well(see pic 49) now your launch templete is created, and this is wha we did untill now(see pic 50), once you create launch templete then it wil say you(see p 51) you eiter dierectly launch instance from this templete or you can attatch to auto scallin group and lauch a instance, now go to auto scaling secion and click on create auto scalling group (see pic 52), now give it name and now it will as for templete so you have to select the templete we have just cerated and from that it will start the auto scalling groups(see pic 53), is ask for one more thing which is version of lauch templete (it mean the above launch templete we created in it we use node in second verstion if you want to use go lang then you can maintain version of you lauch templete to do that by going here you need to modify the templete and once you modify a new version is created (see pic 54)), so right now we select defalut version in autoscaling group(see pic 55) and then click on next btn now you have to specify instance type attributes, what kind of machines you need  do you need it based on vcpu memory matric(s p 56) or do you need based on some instance type(s p 57) either is fine but right now we go with instance type because t2.micro is free tear eligiable which mean you can use it free for 50 hours so select that(s p 59), now (s p 59 ) there are two instace type sport instance are silghtly cheaper but they go down a lot, so on-demain is fine but litte more expensive, (see pic 60) now selcte avelibility zone mean in the same data center or in same region they have multiple avalibility zones lot of time if one goes down then other is avalable(we should select one other wicse if we select two then it some time teminates our instance form form zone and run in diff zone so if we do not want that then select one only)(see p 61) then click on next, now load balancing so click on attach to new laoad balancing (see pic 62), load balancer of multiple type(application load balcncer, network loadbalcer(if you are doing tcp udp low level thing then)) , here we need application load balcnce so select that, then select scheme internet-facing(see pic 64), (whenever req comes to load balancer then it should comes on port 80 which is http and then letter on we add 443 as well which is https)----->
// ------> then in listen and routing select port 80 and you have to specify target group(where we attached our instaces) to tell load balancer so to go to this target gorup so here we create new target group give name (see p 65)(when your auto scalling group start new instance thne it attatch this specific target group and your load balancer put req there ),then ignroe helth setting, ignore addintional settings and click on next, now it ask for group size like how many machine you neeed let say right now we just need 1, min also 1, max also 1(see pic 66), eventually we auto scale, (see pic 67) right now select no auto scaling policies, no policy on instance management which is fine(see pic 68) - it mean how does the machine starts how does the machine end should i start mahine before terminating other one we do not care mix behaviour is fine, so then click on next we do not need any notification so again click on next we do not need any tags so again click on next, then review what have we done then click on cerate auto scaling group

// while creating ASG we did first attach templte, created load blacer, and created new target group

// QNA 
// is there any way by which via code we can create this entipre process automatically - by cli or via teraform may have way to do this

// ASG successed now goes to ASG section copy public DNS and add port and check is it working or not, so it is working, to increae machine of our auto scaling group (see pic 69,70) we added 4 instance all four have someting expose on port 3000 on the internet, the probles stall remain same we are not accessing via load balancer but we fix it very soon

// if you do not want to do all these process that we did above then use elastic beanstalk(see pic 71) it will create all above things for you

// we can have multiple target group, you can also say that send req to fist target group to 30% of time and for 2 50% of time


// now let see our load balancer is workign or not, so goes to load balancer section and you might see one or two over here(see pic 72) , we click on we just cerated recently so here you find the load balancer url(see pic 73) right now that url not working(see pic 74) then mean here also we have to add inbould or sequrity rules(see pic 75,76) so add two port one for shh and 3000, if now i refresh load balancer you can see pic 74 worning goes away now if i goes to this load balancer url it sill says 503 bed getway(see pic 77), my auto scaling group has 4 machine runing then why my load balancer is not able to send the requset let's try to dibug, so goes to load balancer and check it sends req to which tartger group, (see pic 78) so any req that comes on port 80 so if i open this target group it says 0 helthy 4 runing, so our 4 instances are attach to this target group but they all are unhealthy because the port they are trying to hit is port 80(see pic 80 ) but our app is ruuing on port 3000 so we can not edit this target group so we have to creaete new target group and update our load balacer to point to the fresh target group so i will go to tharget gorup here i will click on cerate target group, right now we choose type as instence(see pic 81) but letter on we give it to auto scaling group so it can attach all the instace here, and now select port 3000(see pic 82) where our app is runing, and give the target group name and evering is fine click on next then it as me that do you want to attact any instance to it(we have already 4 instance running) but i will not attach from here i will give this template to auto scalling group it will automatically attach them and click on create target gorup now i have a fresh target group with 0 instacees becasue i did not attached any one, but now how i will tell auto scalling group that now you can attach all the instaces to this new group so goes to autoscaling group, (see pic 83) click on edit button in load balaceing and change the new target group (see pic 84), now i attach this auto scaling group to my newly created target group which mean all of my instances which are ruunig will now goes to the newly created target groupd, so now check in your taget group (see pic ) port got changed and all 4 are healthy and running, now, now lets goes to load balancer and chage the target gorup there as well (see pic 86) so in lisner section you have to change it,----------->
// ------> (see pic 87) you now reached to listener rule section here also click on edit rules and (s p 88) here attach new target group here you can add multiple target group  and also specify how much weight each target group should handle, and click on save changes, now our app is available in load balancer url and now it is working on load balacer url(see pic 89) defalut port is 80 so we do not need to specify defalut port, now only 1 thing left how to point this load balancer url to my own domain name

// Note: you do not only have load balancer for just only auto scaling groups , you can create 5 ec2 machine by your selfs and attach them to a load balancer either way \

// now lets attach domin - the thing you have to do is goes to your load balancer, select the once with you want to attach new domain, (see pic 90) and add listener on port 443(becasue this port listen on https (se p 91)) so i add the port and select fresly created target group(see pic 91) so select that, but whenever you have https connection then you need a certificate attached to your domain(last time we use certbort in past), but right now we used aws's because it has it own, and for two reason you should use aws 1)  more authentic then cert bort 2) it will automatically renew, it will automatically tell you that hay your certificate is expiring let me renew is so on.. but cert bort do not do that you have to there manually, so if see in secure listener setting (s p 92)  here is say do you have certificate for this and we do not have so we have to generate new certificate for my domain name, we are tring to attach our own domain to load balancer before we do that we have to create certificat for our domain name so click on request new ACM certificate(see pic 93 this is similar to ruuing certbort on our terminal ) but you have to prove to aws that i own this domain name but how would you prove to aws, (see pic 94)now let's do that first click on next, (see p 95) here put your domain name, and here it give you two option to prove that this domain is your(see pic 96) dns is the easiest one so select that and any key algorithm is fine so select one of them and then click on reauest and when you do that if you refresh the page you get issued certificat with pending status(s p 97) so i need to prove to aws that i own this spcific domain name, so go to in taht certificate(see pic 98,99), and aws tell you that please add these dns entries(see p 99) over there it says CNAME name should point to CNAME value it it does then i assume or you have prove to me that you own the domain so i have to add these values to my dns provider where you buy your domin from (s p 100,101,102,103,104,105) so this is how i can prove to aws that i own this domain and in two mins your certificate get isssued, (see pic 106)so once it get issed for that domain wheere we want to host website so selct certificate and click on add button now i last thing is remaing is domin linking, (see pic 107)go to your dns provier and in dns section and new record and link load balancer url there and  click on save and your app is runing, (s p 108,109) it is working

// Note - once certificate get issued you can delete the entry that you have cerate to issue it for aws

// so the now last experiment that we want to run is it is an auto scaling group but we havet done any auto scaling yet you have done scaling but not auto scaling because you have seleceted 4 and you got the 4 instance how it is auto scaled, so to see how it goes up and down click on edit(see pic 110), so here(s p 111) select min 1 max 5 and desired capacity 1 so it will down scale to a single machine which is fine and save, so if we go to instance management now you can see only one is in service, but how do i now auto scale, you goes to auto scaling section here(s p 113) where you can add easy policies what are the easy policies you remember(ig cpu usges goes above soming then do somting and if it is goes down somging then do somting), so click on create new dianamic scale policy,(see pic 114) (so there are there type policies step scaling is little complecated, target scaling is easy one so let's select that one), (s p 115) then average cup utilication should be 50% if it goes above it please increase no of server if it goes bellow it please decrease it, this is the most easyest way to create most busic auto scaling policy and click on create if you want ot add it so after this policy test it that is it working or not (see pic 116), and when it auto scale up or down scale it pust all log in activity tab in auto scaling group  (s p 117)


// now one more thing that we need to discuss there are ade hock phase of scaling lot of time the matric that you want is not the cpu usges but you know the some rendom matric, that i need 5 server because my queue length is 3 or i need 100 server right now because of somematic that i dicided my my self but two type of things we discussed if you createing a chess like application then this final number is 80,000 then there is 80,000 websocket connection then you probabally need 10 servers so the matick is somting you own in this case that i want 10 machins because i have 80000 users so if you have an architecture like this how can you tell the auto scaling group that plase increase the size to  20 machine to 30 machines, and the ans is you can write the provess in whatever language you want that you know connects to autoscaling group and tells it that hey please increase the capacity or please decrease the capacity the code in node js somthing like this(see pic 118) aws has somting called aws-sdk package and it is in all language available, so you can send the req from various places as long as you have some secrate password to your account, so before you do any of these thing you have to create new user in this sectiion(see pic 118) called security credentials and create a new user which has some restricted permitions,(see p 118 here need key and secrate) you can also create these secrate key and  secrate for your own user as well then what do you need in the end is secrate key and  secrate to connect to your auto scaling group and change thins there change the desire capacity but you should not do it for your own user, you should create a fresh user which has some restricted permitions and use the access key and secreate for that so if user account get compromised only your auto scaling group get compromised not the hole thing, so crate new user(see pic 121,120) with only one permition Auto Scalling Full Access it has full access to my auto scaling group but noting else so it can create image in s3, so it can delete nay image form s3, and for accss key and secret (s p 122)select that user and go to security credientials and cerate new accss key and secret that you can use in code (see pi 118,119) so here now add you auto scaling group name and based on your requiremtn and your logic increase and decrease new instance


// Note - now clean up the resources
// 1 make desired capacity 0 (see pic 124)
//  but you still get chaged for images load balancer so you should clean up everyting
// delete load balancer
// then target group 
// then lauch templets
// then images
// and your first ec2 machine that we cerated to create image


// add ddos protection with cloused fair other wise in auto scaling you get to charged lot 

// to add letest code in you instance - first decrese capacy to 0 and then increase so old instances get down which has old code and when it get started again you have starter script so over there you are running git pull so that how , and you can do this above thing with ci cd pipeline as well

// load tasting - use load-test library for ti so it send bunch of req to gether