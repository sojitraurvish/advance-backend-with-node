// what is vertical scaling mean you increase machine size and with  cluster module create as many worker process you want with as per cpu cores, but after some point you can increase the size of your machine then you have to goes to vertical scaling

// there is limit that how much you can vertically scale 
// you can get as much bigget machine you want but here some problem you have very big machine and everyone is sending request to the same machine then it is a problematic 
// 1) you are being dependent on a single machine if machine goes down you have single point failier
// 2) you are telling all billion user to connect to same server what if some one in us someone in india your machine can only be in us and hance indian people have lot of latency so idially you should horizontally scale 

// what is horizontally scallin mean (see pic 3)
// that you should have bunch of servers not just one  each server is very powerful that is fine each server can have node js running in cluster module that is fine as well, there should be multiple server and when user goes to your website the requset should first goes to load balancer which you know machine in it self which as name suggest(balances the load) so it send  the request to one of many servers that is how you horizontally scale, why it is called horizontally scaling - because theorotically you can say i am adding more machines horizontally that why

// will see how to scale horizontally by some aws constracts something called auto scalling group, the easyest way to do auto scaling is i will have 1 machine innitially but as the load goes above 50% on this machine i will add another machine and now load should be probebally bye 25% because i doubled the number of servers then let say the more traffic came and load again become 50% (what do you mean by load - cup usges) then i scale again one more time and net load on system is 33% if again it goes above 50% i will scale up more and if it want down to 25% then we decrease the no of servers this is the easiest way to auto scale based on either incomming bandwidth, or incomming request count, or avg cpu usage of your cluster so these are easier ways to scale where you do not have any ade hock logic - mean somting like these architecture(see pic 1)  for example yesterday we discuss if we have realtime game like chess, then you have bunch of machine let say this is (see pic 1 S1) aws machine which has 6 cores running in cluster module so i have 6 process running in S1, 6 in S2, 6 in S3, 6 in S4 and all of these has persistent websocket connction ot the user, and let say you know in node js if you are creating websocket connections, 10,000 is good no if you have more than 10,000 users you will start to create issues or you start ot face issues you are not dependent on cup usages you can dependent on some number that how many people are currently playing if 88,000 people are currently playing then you want 9 machines, because more than 10,000 people comes on single server it a problem, so this is ade hock scaling and there is matic that you own , the above matric () 1) incomming bandwidth 2) incomming request count 3) avg cpu usage fo your cluster) that we talked about that somthing that the clould providers automatilly get you do not have to do lot of efforts when you scaling like this you just need to tell the cloud provider in this case aws these are my rules(1,2,3 just above) if the incomming bandwith average above 50% you increse no of server and if it goes down you decrease no of servers, but (see pic 1 architecture) this is more ade hock logic and the cloud provider does not know this, there is not option to see current open connections or no req per second you can see if you are goring through load balancer or(you do not need this no cont if it si paytm like app archetecture see pic 2) ------>
// ------> you can have aggregate each server connection(see pic 2) req no and you create a matic which tell ASG in aws to increase 10 servers i now have 88,000 people might that is lot of my websocket servers can not  handle that ade hock scaling mean you own the matic based on that you scale and this is your system dependent matic, what can be another matic is youtube like plateform that transcode the video as people comes to your app and upload video in 1080p then this video should be conveted into three videos 1080p, 720p, 380p this process can take a really long time if i upload 2 hours video on youtube it can also take 2 hours sometimes, here also if people comes to your website and uploads the video then you have to starts a machine for them that are transcodding there videos if three people have uploaded the videos then you might have three servers that are transcoding there videos,here a good matic on which you should decide the no of machine should be up si queue length the length of the queue which stores the currently pending videos as people upload to your plateform you push there video to a queue like this person has uploaded the video what idially happens is you upload a video to a cloud provider you upload your official video whatever your video editor gave to you it actually goes to s3 or some object stores(youtube own by google google will never use amazon) so first from dashbord your video is get uploaded to some object store once this is done then this location of the video is get pushed into queue like there is this video id that harkirt had posted id :1 user_id:2 and video location(s3 or any object store) this is the first thing that goes into the queue so this queue is getting filled, why am i doing queue architecture, because thus system can tell directly to worker that please transcode, this worker may be bussy transcodding someone else video so you asyncly handle this task, you send it queue first and then this queue lenth keeps on incresing let say there is spike everyone start to upload then is queue lenth become 100 and this is a matic based on which you scale now ovv now 100 vides are pending  we should probebally scale up to 50 servers so at least 50% videos give transcoded in next hour and rest 50% in next to next hour so here again it is an ade hock matic where the queue length which decides the no of currently active workers so this is the third way -----> 
// ----> today we will implement both of this architecture(1 paytem, 2 chess app)  -  how would you do auto scaling both these matrix either ade hock matic (matich from our side) or a simple matic like cpu usages how you scale up and down on aws now let's do that

// ASG (see pic 3) but this is not the cleanest approach to do it there are bunch of ways to auto scale the better way to auto scale usally using containers(see pic ) you do not start machine again and again, you have some either very big machine somewhere where you start containers and as load increases you start more and more containers here container are more light weight and easyer to start compare to want we are doing right now what we are doing right now is letrally starting multiple ec2 machines , if you goes to your aws console then you can see multiple machine get added here it's very row way to auto scale this is how people auto scale until containers ware now there now container way is more light weight which mean more easyer to start(ec2 machine takes 20 to 30 s to start) so that is why this is not an best approach  but this is great approach if you want to avoid cueber natic if you want to aviod containers if you do not have containers in your company right now then the row way to auto scale is this in aws it is called auto scaling groups, but auto scaling it self requires a bunch of things you are starting a bunch of machine this machine need to have node js installed, this machine need to have your code that actully needs to excicute(js or ts code), so you have to some how create a snap short of your machine, like this is the machine that i want to bring up or bring down as my scale increases so there are some buzz words to know before we actully procide(see pic 3) and these are aws ASG specific buzz words but you will find them diff in diff cloud provider either with diff name or with the very similar name, the first buzz word to know is someting called image or amezone machine images (AMI) is snape short of machine from which you can create more machines, what are we going to do today we will take simple node.js app and the first thing we will do is we will create first ec2 machine and inside that we will install npm node and get our code form git hub then we will create a snap short of it, we create and image from this ec2 machine this is the first buzz word to know (what is a banifit of creating an image - if you have an image then you can create multiple ec2 machine from it you do not even need auto scaling group even without it, even if you just has ec2 machine from which you created and iamge now you have snapshort of that machine so you can keep recreating this machine ----->
// ------> which will have npm, node, code what ever is the current state of machine(from the machine you created an image) the file system, all the libraries that ware installed you have a snapshort of it ) and why is this useful because using this snap short we can create more ec2 machine  and this snapshort we have to give autoscaling group so that the auto scalling group knows that is how i will start my new servers as scale goes up, so that is the first thing to do, what is the second thing to know load balancer - as you have multiple machines then they will have there own ips, domain names but your end user usally sees what your user sees api.100xdevs.com what is that mean  is this req(api.100xdevs.com) need to be either handled by single machine or someting else it is not neccesory that load balancer as single machine but something needs to handle this and route the req to one of many machines that exsits based on the load and hance this thing is called load balancer , can i create load balancer my my self can i just start and ec2 machine which takes req from user and routes them to various machines based on the load of the machines yes you just do not need to use aws ec2 machine, you can start ec2 machine start some short of proxy implementaion there and that can handle incomming req and forword it to one of them and nginx will let you do that so if you want you can create your own, but usally you differ this to aws why because it is a fully manage service which mean you do not need to worry about scaling it ever, it might happen your load balancer becomes a bottle nack what if 1b users on your website they can be handle by single machine you have 1000 machines to handle a 1b request but you have a single load balancer the this will be a problem so aws thankfully spends a lot of time and effort here because they knows that this is load balancer so this will be the first entry point for all the users so they make sure LB scale up and down have machine in various regions so whenever your load increases due to whatever resons then it will be able to handle those incomming reqs that is why it is good idea that you should use load balancer from aws rather than creating your fresh machine and deploy worker, Target Group -  this is very aws specific, so this might not exits in any other cloud provider, a group of ec2 instances that are load balancer can send a req to, so whenever you start a load balancer ------>
// ------> how do you tell it that if you are recieving a request then please send it to S2 machine you tell it that you are part of this target group then you attach every machine that here to that target group, you tell that you are part of target group t1 this how aws expects you to link a load balancer to your machines, it(LB) says do not give me actual ips your machine or actual ids of your machine give me a target group the target group and the target group what will be connected to various machines it just creates an easy distinction if you have the more target group, then if we have another load balancer where also i can say you also route all the req to target group t1 , anther place where this will be useful is when you create an autoscaling group auto scaling group whenever it adds more machines it can simpally tag those machines with t1 target group so that load balancer can send the req them, we comes to this very soon but buzz word to keep in mind is target groups which let you attach two instances(machines) and whenever you start a load balancer you tell the load balancer that whenever you are recieving req on port 443 you will be forwording it to this target group. lastly, A Launch Template - a launch templte  that can be used to start new machines, you might say that is was an image was image was that only that you create an image and using this image you can start multiple machines what is a templete, the thing is image just takes a snapshort of your machine, it does not takes snapshort of ovv these are the ports are currently open in this machine it does not attach that storage in this machine was 8gb and bunch of other things for example when this machine starts this script should run node index.js should run or pm2 start index.js should run all of this things can not be the part of the image, image only is file system and current snapshort that it looks like this  it has these libraries and these folders a launch template has the image also that this is the image that we are starting or this is the image that we want to launch with otherthings and this is the sequrity group of it what is sequrity group - what all incoming req are posible this is the key pair that can acces this machine, this is the start code - so when the machine starts we need to run pm2 start index.js in this machine so and and so forth that is what a launch template is so there is big diff between image and templete, -------->
// --------------> when you start an instance by using just an image then you have to tell explecitly that this new instance i am starting  this is it's security group this is the space it need to have vs if you have lauch templete, every machine that starts with same sequrity groups same key pair that connect to it sam command that run and start node process so that is last thing

// Note: Plase make sure you get rid of all your resouces after this -> because when you create ec2 machine and then forget them but one thing able auto scaling group is you tell ASG that bro i need 10 machines and if you goes to your aws dashbord and if you stop 10 machine then it will restart them and it will chanrge you lot so plase make sure at the end you clean up fully properly, at the end go here and delete all the images form here(see pic 5), go here and delte auto scaling group from here(see pic 6) and not just stop your ec2 machine , if you just stop your ec2 machine then ASG will restart it

// now let's start to deploy an application

// so first step create image so from which other instance can start and for which you have to first create instance of your own so now step 1 is launchin an new ec2 instance(see pic 7) in which you will bring node and codebase(code base with clutser module see pic 8,9) , once you click on launch a instance you get form to fill up give it name week-22-asg-class(see pic 11), let me select ubanto as os(s p 12), instance type can by anyting does not matter t2.micro should be fine(s p 13) because does not matter with what type you create machine becasue when you are spinning up more machines you can always select more big machine so t2.micro is fine, key pair(let you connect to the aws machine whenever your machine starts somewhere one the internet you need to connect to it and for that you need some sort of password, you do not want any one other on internet to be able to connect it, this key pair is easyest way to connect with your server with ssh protocol, so from your machine you can connect to your server with ssh(see p 14)) so here ceate new key pair or select an existing one(create new give name(s p 15) and at the end you get the pem file that you need to use while connecting see pic 14 hiligheted are at where need to use pem file one more thing you have to do is you have to change the permition in this file otherwise you wan't be able to use this file to login via ssh so this is the command you have to run (see pic 17)), you can create sequrity group here(see pic 18 )if you do not it's fine, at time of auto of creating auto scaling groups you can override it. just leave defalut here allowing ssh traffic from anywhere that mean after this machine get started i can connect it via ssh, size does not matter you can change letter on so take defalut(s pi 19) and now click on lauch instance and that will start new machine, now what are the next steps i need to go to this machine install node js clone my code these are the two thing that i have to do before i goes to the image creation, so wait for this machine to start, now let connet to machine but how do i connect to it, fist copy the public dns or ip does not matter(se p 20), open the terminal in your pc  and run the cmd ssh -i <your temp file name(with location)> ubuntu@<the address>(see pic 21), now you are connected to machine via terminal that you just started, now instll node js, there are many ways(but follow this digtal ocean guid see pic 22) but plase install using nvm it is batter way

// (s p 23) this cmd install not only node but also nvm and it will also give you export that you just need to copy and past(see pic 24) and you have access somting called nvm, now you can use nvm to install node js now wirte cmd nvm install node now node js is installed now next step is cloing a repo, go to repo copy https url not ssh and then run command git clone, after cloneing go inside that folder and try runing cmd npm install and cmd npm run start and check if it is working or not, i can also test it is working or not by fist goes to security group here(s p 25) and openning port 3000 the problem is 3000 is web specific and until we did not added inbound rules(s p 26) for port 3000 so just to check twice(if you find logs in terminal then you app is working) then select security group here(s p 27) and edit the inbound rules(s p 28) right now i have only 1 rule i am allowing ssh access form anywhere, so anyone can hit  the port 22, now i add a rule that says anyone can hit the port 3000 as well because the port 3000 where the application is run check index.js app.port(3000) fucntion so add new inbound rule (see pic 29, 30) for port 3000 and now if i goes to ec2 machine and try to hit my machine on port publicDNS:3000  you can see your code is runing(see pic 31) now you app is deployed on internet but it has bunch of problems, so P1)  url is very ugaly beacause port at the end P2) is it says not secure here(see pic 32), P3) it is not auto scaled yet if lot of people hit there it is scrud so that we will fix, so now step 1 is done which is installing everything in your machine, step 2 is creating an image out of it so slecte the instance for you want to create a image > then selct actions >  go to images and templets > click on create image (see pic 34) which will create a snapshort of your instance at current point(so if at current point my instance has folder called week-22 then my image will also have it and the any machine i start from this image it will also have that ), (see pic 34,35) give image name, size 8 gb is fine if you want you can increase decrease it and click on create image, step 3 is creating auto security group for your final auto scaling group, you final auto scaling group has some inbould rules thins that you want to open up , click on securiy group and create new security group, give name, desc,(see pic 36,37) ---->
// ---->(see pic 38) add inbould rules create two rules for port 3000, and 22(for i can ssh into the final machine and see what is happening ) and here at port 3000 you can add more strick rule that only allow traffic from  load balancer to the instance but why only two ports only 22 is for us to connect and 3000 is for load balance to connect but here can make port 3000 restricted to world only open for load balancer right now we are not doeing but you can do that as well and click on create security group to create it

// recape  whenever you create auto scaling gorup it need few things 1) image (when i start the machine what should it look like you can be like image would be ubantu image you can start auto scaling group with jsut ubanto with no code in it but what do we want that when the auto scalling group starts it should start this application in it so we say no 1 i will create normal ec2 machine we added node js library here  and we create an image form this ec2 machine so that we can give this spcial image  to the auto scallin group that take this the ubantu, code is here, node js is also here so image with these three thins what we give to auto scallling groups, could we have just given the ubanto yes you can just give ubantu and in your start script of the autoscalling groups you will be like fist i install node js, then i will do git clone, then i will do npm install, then i do npm run start we can also do this if you want to avoid creating your custom image you can always just use and ubantu image and in your starter script(starter script runs when machine starts) is where you can write this all of the code or you can be like that this is my custom image this already has ubantu and code, and  node js so now i will run npm run start in my starter script why this approach is better, because now whenever machine starts it does not need to install node js you can simpally run npm run start here in starter scrpt but becore that you can add one more step here which si git pull to get letest code and then run npm run start)

// now i want to create a launch templete  what is launch templete go to launch templetes(see p 39) as the name suggets